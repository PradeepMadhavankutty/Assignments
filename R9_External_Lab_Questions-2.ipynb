{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPJHV_CIn6Bg"
   },
   "source": [
    "# U - Net\n",
    "## Problem: \n",
    "\n",
    "Seismic data is collected using reflection seismology, or seismic reflection. The method requires a controlled seismic source of energy, such as compressed air or a seismic vibrator, and sensors record the reflection from rock interfaces within the subsurface. The recorded data is then processed to create a 3D view of earth’s interior. Reflection seismology is similar to X-ray, sonar and echolocation.\n",
    "\n",
    "A seismic image is produced from imaging the reflection coming from rock boundaries. The seismic image shows the boundaries between different rock types. In theory, the strength of reflection is directly proportional to the difference in the physical properties on either sides of the interface. While seismic images show rock boundaries, they don't say much about the rock themselves; some rocks are easy to identify while some are difficult.\n",
    "\n",
    "There are several areas of the world where there are vast quantities of salt in the subsurface. One of the challenges of seismic imaging is to identify the part of subsurface which is salt. Salt has characteristics that makes it both simple and hard to identify. Salt density is usually 2.14 g/cc which is lower than most surrounding rocks. The seismic velocity of salt is 4.5 km/sec, which is usually faster than its surrounding rocks. This difference creates a sharp reflection at the salt-sediment interface. Usually salt is an amorphous rock without much internal structure. This means that there is typically not much reflectivity inside the salt, unless there are sediments trapped inside it. The unusually high seismic velocity of salt can create problems with seismic imaging.\n",
    "\n",
    "### Data\n",
    "The data is a set of images chosen at various locations chosen at random in the subsurface. The images are 101 x 101 pixels and each pixel is classified as either salt or sediment. In addition to the seismic images, the depth of the imaged location is provided for each image. The goal of the competition is to segment regions that contain salt.\n",
    "\n",
    "#### Source: \n",
    "https://www.kaggle.com/c/tgs-salt-identification-challenge\n",
    "\n",
    "\n",
    "### Note: \n",
    "Accept the terms and download data from the above link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3OKrTNUtn-od"
   },
   "source": [
    "### Aim: \n",
    "\n",
    "Implement U-Net neural model architecture in keras to solve this problem.\n",
    "\n",
    "\n",
    "In this, you are asked to segment salt deposits beneath the Earth’s surface. Given a set of seismic images that are 101 x 101 pixels each and each pixel we need to classify as either salt or sediment. Our goal is to segment regions that contain salt. A seismic image is produced from imaging the reflection coming from rock boundaries. The seismic image shows the boundaries between different rock types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUOAF8WWoA1x"
   },
   "source": [
    "### Broad Steps:\n",
    "\n",
    "1. Download the dataset\n",
    "2. Upload to Drive\n",
    "3. Import from drive to colab\n",
    "4. Load the images and create training data.\n",
    "5. Build U-net Model\n",
    "6. Train your model.\n",
    "7. Check the validation accuracy and plot sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "b7TLNzB6oDep"
   },
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6qiCvW9coD_E",
    "outputId": "dea42448-4c8d-4571-9620-17c0795144ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#Mount drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4IaUgploION"
   },
   "source": [
    "### 1.Set your project path where you have your data and related files for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0tPkbXWyoK-z"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/My Drive/AIML/R9-ExternalLab')\n",
    "project_path = '/content/drive/My Drive/AIML/R9-ExternalLab/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Kxy5LexhoLMT",
    "outputId": "8f6288fd-c200-455d-9f0a-79984cd246b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/AIML/R9-ExternalLab'"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g15qSXZSoSuL"
   },
   "source": [
    "### 2. Set the necessary parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dzrz2Or_HdxR"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3IEaEnuzoW2a"
   },
   "outputs": [],
   "source": [
    "im_width = 128      #width of your train image\n",
    "im_height = 128     #hight of your train image\n",
    "#border =        \n",
    "path_train = project_path + 'train/'   #Path for your train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uZQyyn55oKW-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3IoS6RCoZPh"
   },
   "source": [
    "# 3. Make directory for train data at in your project/lab folder.\n",
    "\n",
    "Hint - use !mkdir function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MRPpQ2QrocBQ"
   },
   "outputs": [],
   "source": [
    "os.mkdir(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRbC22DGoeb7"
   },
   "source": [
    "# 4. Extract your train images to the train directory you have just created above. \n",
    "train.zip and test.zip files available at your google drive/local system.\n",
    "\n",
    "As a good practice - Upload or copy the data at your project path folder.\n",
    "\n",
    "Make sure you are providing the right project_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5BtPznFbogjG"
   },
   "outputs": [],
   "source": [
    "#For simplicity we have added the required code here.\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(project_path + 'train.zip', 'r') as zf:\n",
    "  zf.extractall('train/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7YZ2prb9oiX4"
   },
   "outputs": [],
   "source": [
    "#The train file have both images and masks with the same names_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zxQ1u8Bvoj1M",
    "outputId": "68a016d9-bc32-446f-c302-8a684dae5c0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images', 'masks']"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwCH81ZWomLA"
   },
   "source": [
    "### 5. Get the list of names of images and masks and name the list imagelist and masklist.\n",
    "\n",
    "Hint - Use os.listdir() funtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Clt5ya_JoplC"
   },
   "outputs": [],
   "source": [
    "#Getting the image list \n",
    "imagelist = os.listdir(path_train +'images')\n",
    "masklist = os.listdir(path_train +'masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4nMsiwe2FQw4",
    "outputId": "cd2826c4-b0cf-4843-8f18-9888705202d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "-vHPQoHVoppK",
    "outputId": "798d2593-9c41-4353-b9b8-052e80b3bcb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7cc53fe88b.png\n",
      "7cc53fe88b.png\n",
      "de08c5a3b7.png\n",
      "de08c5a3b7.png\n"
     ]
    }
   ],
   "source": [
    "#Test your list names by printing some of the names as given below.\n",
    "print(imagelist[-1])\n",
    "print(masklist[-1])\n",
    "print(imagelist[10])\n",
    "print(masklist[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WUZw-kSKoqIr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1t8raFq5ous8"
   },
   "source": [
    "# 6. Read and test your images and respective masks.\n",
    "\n",
    "Hint -\n",
    "\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "plt.imshow(cv2.imread('path of image'))\n",
    "\n",
    "plt.imshow(cv2.imread('path of mask'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1gGpCSPpovvH"
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "9E-McW2toyaR",
    "outputId": "e327d0c0-d02d-42af-873d-aa5e655b8dd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc2affd6be0>"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIj4UgUCgiP8HBaEJpWb5O0IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.imread(path_train+'images/'+imagelist[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "tpwBvb93HD-L",
    "outputId": "a170953e-df2b-4ad4-bc9e-f1193e68a9f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc2afd1a470>"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUEgqGRSSSgaFpJJBIan0f9K91J9vS9YsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.imread(path_train+'masks/'+masklist[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qFeroMNoylc"
   },
   "source": [
    "# 7. Create your training data.\n",
    "\n",
    "Hints - \n",
    "\n",
    "image_path = os.path.join(project_path +'path of your image directory' +n )\n",
    "\n",
    "mask_path = os.path.join(project_path +'path of your mask directory'+n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WmFDmIOjKTDL"
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gJbRVbL4o1qT",
    "outputId": "83c083a6-f039-4ae5-f323-54a2a150d81a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get and resize train images and masks\n",
    "def get_data():\n",
    "    X = np.zeros(((len(imagelist), im_height, im_width, 1)), dtype=np.float32) # Create an array for image\n",
    "    y = np.zeros(((len(masklist), im_height, im_width, 1)), dtype=np.float32) #Create an array for mask\n",
    "    \n",
    "    for n in imagelist: \n",
    "        k = imagelist.index(n)\n",
    "        ##Add image_path\n",
    "        image_path =  os.path.join(path_train+'images/', n)\n",
    "        ##Add mask_path\n",
    "        mask_path = os.path.join(path_train+'masks/', n)                                                          \n",
    "              \n",
    "        # Load images and resize to (128,128,1)\n",
    "        img = Image.open(image_path)\n",
    "        img = img.resize((im_width, im_height))\n",
    "        resized_img = np.reshape(img.convert('L'), (128,128,1))\n",
    "        # Load masks and resize to (128,128,1)\n",
    "        img = Image.open(mask_path)\n",
    "        img = img.resize((im_width, im_height))\n",
    "        resized_mask = np.reshape(img.convert('L'), (128,128,1))\n",
    "        # Save images\n",
    "        X[k, ..., 0] = resized_img.squeeze() / 255\n",
    "        y[k] = resized_mask/255\n",
    "    print('Done!')\n",
    "    return X, y\n",
    "    \n",
    "X, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "uZCM1hyBLUma",
    "outputId": "05501607-b95b-4de1-f7ee-58c76e5cc779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 128, 128, 1)\n",
      "(4000, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "#Check the data\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kt6zXJPHo35y"
   },
   "outputs": [],
   "source": [
    "# Split train and valid\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "mh_SRFGro6wr",
    "outputId": "307c6300-9ee2-44d0-e452-78b8fe5d40d1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEuCAYAAAC9NwejAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhUKhUKwBlGQpFAqFQqFQrAH+H/4I4FCUuKdNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test your data whether it looks fine - Random check\n",
    "import random \n",
    "# Check if training data looks all right\n",
    "ix = random.randint(0, len(X_train))\n",
    "has_mask = y_train[ix].max() > 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "ax[0].imshow(X_train[ix, ..., 0], cmap='seismic', interpolation='bilinear')\n",
    "if has_mask:\n",
    "    ax[0].contour(y_train[ix].squeeze(), colors='k', levels=[0.5])\n",
    "ax[0].set_title('Seismic')\n",
    "\n",
    "ax[1].imshow(y_train[ix].squeeze(), interpolation='bilinear', cmap='gray')\n",
    "ax[1].set_title('Salt');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "em5jGV93o8qR"
   },
   "source": [
    "### 8. Define loss and dice_coeff function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Uyxf8uhQpA78"
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "    return numerator / (denominator + tf.keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EmCjyEUUpA_V"
   },
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - tf.log(dice_coefficient(y_true, y_pred) + tf.keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ofKZA-9pDVJ"
   },
   "source": [
    "### 9. Build and compile UNet Model for your data.\n",
    "\n",
    "Hint - \n",
    "You can install and use segmentation models from this github repository.\n",
    "\n",
    "#Install segmentation models\n",
    "\n",
    "!pip install git+https://github.com/qubvel/segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "DFh1hMp6Lr-L",
    "outputId": "fa55aafa-d1d8-4254-c374-0a18b84d820a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/segmentation_models\n",
      "  Cloning https://github.com/qubvel/segmentation_models to /tmp/pip-req-build-5dh8dyln\n",
      "Requirement already satisfied (use --upgrade to upgrade): segmentation-models==0.2.0 from git+https://github.com/qubvel/segmentation_models in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models==0.2.0) (2.2.4)\n",
      "Requirement already satisfied: keras_applications>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from segmentation-models==0.2.0) (1.0.7)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from segmentation-models==0.2.0) (0.14.2)\n",
      "Requirement already satisfied: image-classifiers==0.2.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models==0.2.0) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (1.0.9)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (1.16.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.0->segmentation-models==0.2.0) (3.13)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->segmentation-models==0.2.0) (4.3.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->segmentation-models==0.2.0) (3.0.3)\n",
      "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image->segmentation-models==0.2.0) (2.3)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image->segmentation-models==0.2.0) (0.6.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->segmentation-models==0.2.0) (1.0.3)\n",
      "Requirement already satisfied: dask[array]>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->segmentation-models==0.2.0) (1.1.5)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image->segmentation-models==0.2.0) (0.46)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-image->segmentation-models==0.2.0) (2.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-image->segmentation-models==0.2.0) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-image->segmentation-models==0.2.0) (2.5.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-image->segmentation-models==0.2.0) (0.10.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image->segmentation-models==0.2.0) (4.4.0)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=1.0.0->scikit-image->segmentation-models==0.2.0) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image->segmentation-models==0.2.0) (40.9.0)\n",
      "Building wheels for collected packages: segmentation-models\n",
      "  Building wheel for segmentation-models (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-lyr7d_6z/wheels/49/cf/46/cbb4bb64518c402aea99df9d466f1081450597e653256bbcf4\n",
      "Successfully built segmentation-models\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/qubvel/segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "noOcDEUXM5sQ"
   },
   "outputs": [],
   "source": [
    "from segmentation_models import Unet\n",
    "from segmentation_models.backbones import get_preprocessing\n",
    "from segmentation_models.losses import bce_jaccard_loss\n",
    "from segmentation_models.metrics import iou_score\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7872
    },
    "colab_type": "code",
    "id": "456utznCpGco",
    "outputId": "8bc421b6-e3c8-494c-ef6d-b3dfee968480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               (None, 128, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn_data (BatchNormalization)    (None, 128, 128, 1)  3           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_69 (ZeroPadding2 (None, 134, 134, 1)  0           bn_data[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0 (Conv2D)                  (None, 64, 64, 64)   3136        zero_padding2d_69[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn0 (BatchNormalization)        (None, 64, 64, 64)   256         conv0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu0 (Activation)              (None, 64, 64, 64)   0           bn0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_70 (ZeroPadding2 (None, 66, 66, 64)   0           relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling0 (MaxPooling2D)         (None, 32, 32, 64)   0           zero_padding2d_70[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn1 (BatchNormaliz (None, 32, 32, 64)   256         pooling0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu1 (Activation) (None, 32, 32, 64)   0           stage1_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_71 (ZeroPadding2 (None, 34, 34, 64)   0           stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv1 (Conv2D)     (None, 32, 32, 64)   36864       zero_padding2d_71[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn2 (BatchNormaliz (None, 32, 32, 64)   256         stage1_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu2 (Activation) (None, 32, 32, 64)   0           stage1_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_72 (ZeroPadding2 (None, 34, 34, 64)   0           stage1_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv2 (Conv2D)     (None, 32, 32, 64)   36864       zero_padding2d_72[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_sc (Conv2D)        (None, 32, 32, 64)   4096        stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 32, 32, 64)   0           stage1_unit1_conv2[0][0]         \n",
      "                                                                 stage1_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn1 (BatchNormaliz (None, 32, 32, 64)   256         add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu1 (Activation) (None, 32, 32, 64)   0           stage1_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_73 (ZeroPadding2 (None, 34, 34, 64)   0           stage1_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv1 (Conv2D)     (None, 32, 32, 64)   36864       zero_padding2d_73[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn2 (BatchNormaliz (None, 32, 32, 64)   256         stage1_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu2 (Activation) (None, 32, 32, 64)   0           stage1_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_74 (ZeroPadding2 (None, 34, 34, 64)   0           stage1_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv2 (Conv2D)     (None, 32, 32, 64)   36864       zero_padding2d_74[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 32, 32, 64)   0           stage1_unit2_conv2[0][0]         \n",
      "                                                                 add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_bn1 (BatchNormaliz (None, 32, 32, 64)   256         add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_relu1 (Activation) (None, 32, 32, 64)   0           stage1_unit3_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_75 (ZeroPadding2 (None, 34, 34, 64)   0           stage1_unit3_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_conv1 (Conv2D)     (None, 32, 32, 64)   36864       zero_padding2d_75[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_bn2 (BatchNormaliz (None, 32, 32, 64)   256         stage1_unit3_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_relu2 (Activation) (None, 32, 32, 64)   0           stage1_unit3_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_76 (ZeroPadding2 (None, 34, 34, 64)   0           stage1_unit3_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_conv2 (Conv2D)     (None, 32, 32, 64)   36864       zero_padding2d_76[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 32, 32, 64)   0           stage1_unit3_conv2[0][0]         \n",
      "                                                                 add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn1 (BatchNormaliz (None, 32, 32, 64)   256         add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu1 (Activation) (None, 32, 32, 64)   0           stage2_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_77 (ZeroPadding2 (None, 34, 34, 64)   0           stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv1 (Conv2D)     (None, 16, 16, 128)  73728       zero_padding2d_77[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn2 (BatchNormaliz (None, 16, 16, 128)  512         stage2_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu2 (Activation) (None, 16, 16, 128)  0           stage2_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_78 (ZeroPadding2 (None, 18, 18, 128)  0           stage2_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv2 (Conv2D)     (None, 16, 16, 128)  147456      zero_padding2d_78[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_sc (Conv2D)        (None, 16, 16, 128)  8192        stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 16, 16, 128)  0           stage2_unit1_conv2[0][0]         \n",
      "                                                                 stage2_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn1 (BatchNormaliz (None, 16, 16, 128)  512         add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu1 (Activation) (None, 16, 16, 128)  0           stage2_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_79 (ZeroPadding2 (None, 18, 18, 128)  0           stage2_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv1 (Conv2D)     (None, 16, 16, 128)  147456      zero_padding2d_79[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn2 (BatchNormaliz (None, 16, 16, 128)  512         stage2_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu2 (Activation) (None, 16, 16, 128)  0           stage2_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_80 (ZeroPadding2 (None, 18, 18, 128)  0           stage2_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv2 (Conv2D)     (None, 16, 16, 128)  147456      zero_padding2d_80[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 16, 16, 128)  0           stage2_unit2_conv2[0][0]         \n",
      "                                                                 add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_bn1 (BatchNormaliz (None, 16, 16, 128)  512         add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_relu1 (Activation) (None, 16, 16, 128)  0           stage2_unit3_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_81 (ZeroPadding2 (None, 18, 18, 128)  0           stage2_unit3_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_conv1 (Conv2D)     (None, 16, 16, 128)  147456      zero_padding2d_81[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_bn2 (BatchNormaliz (None, 16, 16, 128)  512         stage2_unit3_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_relu2 (Activation) (None, 16, 16, 128)  0           stage2_unit3_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_82 (ZeroPadding2 (None, 18, 18, 128)  0           stage2_unit3_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_conv2 (Conv2D)     (None, 16, 16, 128)  147456      zero_padding2d_82[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 16, 16, 128)  0           stage2_unit3_conv2[0][0]         \n",
      "                                                                 add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_bn1 (BatchNormaliz (None, 16, 16, 128)  512         add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_relu1 (Activation) (None, 16, 16, 128)  0           stage2_unit4_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_83 (ZeroPadding2 (None, 18, 18, 128)  0           stage2_unit4_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_conv1 (Conv2D)     (None, 16, 16, 128)  147456      zero_padding2d_83[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_bn2 (BatchNormaliz (None, 16, 16, 128)  512         stage2_unit4_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_relu2 (Activation) (None, 16, 16, 128)  0           stage2_unit4_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_84 (ZeroPadding2 (None, 18, 18, 128)  0           stage2_unit4_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_conv2 (Conv2D)     (None, 16, 16, 128)  147456      zero_padding2d_84[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 16, 16, 128)  0           stage2_unit4_conv2[0][0]         \n",
      "                                                                 add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn1 (BatchNormaliz (None, 16, 16, 128)  512         add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu1 (Activation) (None, 16, 16, 128)  0           stage3_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_85 (ZeroPadding2 (None, 18, 18, 128)  0           stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv1 (Conv2D)     (None, 8, 8, 256)    294912      zero_padding2d_85[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn2 (BatchNormaliz (None, 8, 8, 256)    1024        stage3_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu2 (Activation) (None, 8, 8, 256)    0           stage3_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_86 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv2 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_86[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_sc (Conv2D)        (None, 8, 8, 256)    32768       stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 8, 8, 256)    0           stage3_unit1_conv2[0][0]         \n",
      "                                                                 stage3_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn1 (BatchNormaliz (None, 8, 8, 256)    1024        add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu1 (Activation) (None, 8, 8, 256)    0           stage3_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_87 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv1 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_87[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn2 (BatchNormaliz (None, 8, 8, 256)    1024        stage3_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu2 (Activation) (None, 8, 8, 256)    0           stage3_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_88 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv2 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_88[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 8, 8, 256)    0           stage3_unit2_conv2[0][0]         \n",
      "                                                                 add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_bn1 (BatchNormaliz (None, 8, 8, 256)    1024        add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_relu1 (Activation) (None, 8, 8, 256)    0           stage3_unit3_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_89 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit3_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_conv1 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_89[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_bn2 (BatchNormaliz (None, 8, 8, 256)    1024        stage3_unit3_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_relu2 (Activation) (None, 8, 8, 256)    0           stage3_unit3_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_90 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit3_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_conv2 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_90[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 8, 8, 256)    0           stage3_unit3_conv2[0][0]         \n",
      "                                                                 add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_bn1 (BatchNormaliz (None, 8, 8, 256)    1024        add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_relu1 (Activation) (None, 8, 8, 256)    0           stage3_unit4_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_91 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit4_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_conv1 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_91[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_bn2 (BatchNormaliz (None, 8, 8, 256)    1024        stage3_unit4_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_relu2 (Activation) (None, 8, 8, 256)    0           stage3_unit4_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_92 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit4_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_conv2 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_92[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 8, 8, 256)    0           stage3_unit4_conv2[0][0]         \n",
      "                                                                 add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_bn1 (BatchNormaliz (None, 8, 8, 256)    1024        add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_relu1 (Activation) (None, 8, 8, 256)    0           stage3_unit5_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_93 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit5_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_conv1 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_93[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_bn2 (BatchNormaliz (None, 8, 8, 256)    1024        stage3_unit5_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_relu2 (Activation) (None, 8, 8, 256)    0           stage3_unit5_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_94 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit5_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_conv2 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_94[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 8, 8, 256)    0           stage3_unit5_conv2[0][0]         \n",
      "                                                                 add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_bn1 (BatchNormaliz (None, 8, 8, 256)    1024        add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_relu1 (Activation) (None, 8, 8, 256)    0           stage3_unit6_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_95 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit6_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_conv1 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_95[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_bn2 (BatchNormaliz (None, 8, 8, 256)    1024        stage3_unit6_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_relu2 (Activation) (None, 8, 8, 256)    0           stage3_unit6_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_96 (ZeroPadding2 (None, 10, 10, 256)  0           stage3_unit6_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_conv2 (Conv2D)     (None, 8, 8, 256)    589824      zero_padding2d_96[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 8, 8, 256)    0           stage3_unit6_conv2[0][0]         \n",
      "                                                                 add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn1 (BatchNormaliz (None, 8, 8, 256)    1024        add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu1 (Activation) (None, 8, 8, 256)    0           stage4_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_97 (ZeroPadding2 (None, 10, 10, 256)  0           stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv1 (Conv2D)     (None, 4, 4, 512)    1179648     zero_padding2d_97[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn2 (BatchNormaliz (None, 4, 4, 512)    2048        stage4_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu2 (Activation) (None, 4, 4, 512)    0           stage4_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_98 (ZeroPadding2 (None, 6, 6, 512)    0           stage4_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv2 (Conv2D)     (None, 4, 4, 512)    2359296     zero_padding2d_98[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_sc (Conv2D)        (None, 4, 4, 512)    131072      stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 4, 4, 512)    0           stage4_unit1_conv2[0][0]         \n",
      "                                                                 stage4_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn1 (BatchNormaliz (None, 4, 4, 512)    2048        add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu1 (Activation) (None, 4, 4, 512)    0           stage4_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_99 (ZeroPadding2 (None, 6, 6, 512)    0           stage4_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv1 (Conv2D)     (None, 4, 4, 512)    2359296     zero_padding2d_99[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn2 (BatchNormaliz (None, 4, 4, 512)    2048        stage4_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu2 (Activation) (None, 4, 4, 512)    0           stage4_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_100 (ZeroPadding (None, 6, 6, 512)    0           stage4_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv2 (Conv2D)     (None, 4, 4, 512)    2359296     zero_padding2d_100[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 4, 4, 512)    0           stage4_unit2_conv2[0][0]         \n",
      "                                                                 add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_bn1 (BatchNormaliz (None, 4, 4, 512)    2048        add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_relu1 (Activation) (None, 4, 4, 512)    0           stage4_unit3_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_101 (ZeroPadding (None, 6, 6, 512)    0           stage4_unit3_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_conv1 (Conv2D)     (None, 4, 4, 512)    2359296     zero_padding2d_101[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_bn2 (BatchNormaliz (None, 4, 4, 512)    2048        stage4_unit3_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_relu2 (Activation) (None, 4, 4, 512)    0           stage4_unit3_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_102 (ZeroPadding (None, 6, 6, 512)    0           stage4_unit3_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_conv2 (Conv2D)     (None, 4, 4, 512)    2359296     zero_padding2d_102[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 4, 4, 512)    0           stage4_unit3_conv2[0][0]         \n",
      "                                                                 add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 4, 4, 512)    2048        add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 4, 4, 512)    0           bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_upsample (UpSamp (None, 8, 8, 512)    0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 8, 8, 768)    0           decoder_stage0_upsample[0][0]    \n",
      "                                                                 stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_conv1 (Conv2D)   (None, 8, 8, 256)    1769472     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_bn1 (BatchNormal (None, 8, 8, 256)    1024        decoder_stage0_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_relu1 (Activatio (None, 8, 8, 256)    0           decoder_stage0_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_conv2 (Conv2D)   (None, 8, 8, 256)    589824      decoder_stage0_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_bn2 (BatchNormal (None, 8, 8, 256)    1024        decoder_stage0_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_relu2 (Activatio (None, 8, 8, 256)    0           decoder_stage0_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_upsample (UpSamp (None, 16, 16, 256)  0           decoder_stage0_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 16, 16, 384)  0           decoder_stage1_upsample[0][0]    \n",
      "                                                                 stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_conv1 (Conv2D)   (None, 16, 16, 128)  442368      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_bn1 (BatchNormal (None, 16, 16, 128)  512         decoder_stage1_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_relu1 (Activatio (None, 16, 16, 128)  0           decoder_stage1_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_conv2 (Conv2D)   (None, 16, 16, 128)  147456      decoder_stage1_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_bn2 (BatchNormal (None, 16, 16, 128)  512         decoder_stage1_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_relu2 (Activatio (None, 16, 16, 128)  0           decoder_stage1_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_upsample (UpSamp (None, 32, 32, 128)  0           decoder_stage1_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 192)  0           decoder_stage2_upsample[0][0]    \n",
      "                                                                 stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_conv1 (Conv2D)   (None, 32, 32, 64)   110592      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_bn1 (BatchNormal (None, 32, 32, 64)   256         decoder_stage2_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_relu1 (Activatio (None, 32, 32, 64)   0           decoder_stage2_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_conv2 (Conv2D)   (None, 32, 32, 64)   36864       decoder_stage2_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_bn2 (BatchNormal (None, 32, 32, 64)   256         decoder_stage2_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_relu2 (Activatio (None, 32, 32, 64)   0           decoder_stage2_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_upsample (UpSamp (None, 64, 64, 64)   0           decoder_stage2_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 64, 64, 128)  0           decoder_stage3_upsample[0][0]    \n",
      "                                                                 relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_conv1 (Conv2D)   (None, 64, 64, 32)   36864       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_bn1 (BatchNormal (None, 64, 64, 32)   128         decoder_stage3_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_relu1 (Activatio (None, 64, 64, 32)   0           decoder_stage3_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_conv2 (Conv2D)   (None, 64, 64, 32)   9216        decoder_stage3_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_bn2 (BatchNormal (None, 64, 64, 32)   128         decoder_stage3_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_relu2 (Activatio (None, 64, 64, 32)   0           decoder_stage3_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_upsample (UpSamp (None, 128, 128, 32) 0           decoder_stage3_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_conv1 (Conv2D)   (None, 128, 128, 16) 4608        decoder_stage4_upsample[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_bn1 (BatchNormal (None, 128, 128, 16) 64          decoder_stage4_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_relu1 (Activatio (None, 128, 128, 16) 0           decoder_stage4_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_conv2 (Conv2D)   (None, 128, 128, 16) 2304        decoder_stage4_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_bn2 (BatchNormal (None, 128, 128, 16) 64          decoder_stage4_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_relu2 (Activatio (None, 128, 128, 16) 0           decoder_stage4_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv2D)             (None, 128, 128, 1)  145         decoder_stage4_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "sigmoid (Activation)            (None, 128, 128, 1)  0           final_conv[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 24,449,876\n",
      "Trainable params: 24,432,530\n",
      "Non-trainable params: 17,346\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Unet(backbone_name='resnet34', encoder_weights=None, input_shape=(128, 128, 1))\n",
    "#compile your model by adding the parameters like optimizer, loss and metrics.\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[dice_coefficient])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eL1lRlXDpIHl"
   },
   "outputs": [],
   "source": [
    "#Get the summary of your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cA4f4oYYpJA6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UYvb8OdpLV5"
   },
   "source": [
    "### 10. Fit your model using model.fit function.\n",
    "Hint - As it might take long time to run. Run it for only 1 or 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "bFw1Yq06pQbp",
    "outputId": "b9f02eb3-87de-4fd6-917c-c3f8eb25b7b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3400 samples, validate on 600 samples\n",
      "Epoch 1/1\n",
      "3400/3400 [==============================] - 1705s 501ms/step - loss: 1.1808 - dice_coefficient: 0.5142 - val_loss: 1.4924 - val_dice_coefficient: 0.5524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2ac603cf8>"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=64,    \n",
    "    epochs=1,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iH34vMq9pSnh"
   },
   "source": [
    "### 11.Predict on val set using model.predict funtion and store in preds_val variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FymN5QpopXW-",
    "outputId": "8e438325-62c9-4115-d729-7cdd9ca88a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 73s 121ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_val = model.predict(X_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uKkpE3vEpXi8"
   },
   "outputs": [],
   "source": [
    "#Get the threshold predictions to look at refined results.\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Pyn3RyCRpaRH"
   },
   "outputs": [],
   "source": [
    "#Plot a sample\n",
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    has_mask = y[ix].max() > 0\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(20, 10))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='seismic')\n",
    "    if has_mask:\n",
    "        ax[0].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[0].set_title('Seismic')\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze())\n",
    "    ax[1].set_title('Salt')\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1)\n",
    "    if has_mask:\n",
    "        ax[2].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[2].set_title('Salt Predicted')\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=1)\n",
    "    if has_mask:\n",
    "        ax[3].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[3].set_title('Salt Predicted binary');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "colab_type": "code",
    "id": "s_h5fcgYpbyR",
    "outputId": "670af037-57c0-414a-c0ec-210c8dd57557"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-dfe4b6676693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_val_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-74ff40d866f9>\u001b[0m in \u001b[0;36mplot_sample\u001b[0;34m(X, y, preds, binary_preds, ix)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'seismic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    636\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    637\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIkAAAJDCAYAAACPEUSwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1440x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if valid data looks all right\n",
    "plot_sample(X_valid, y_valid, preds_val, preds_val_t, ix=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oasnhlaDpeU0"
   },
   "source": [
    "If you are getting good results- Congratulations.\n",
    "If you are not, try to explore what might be the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yllJR0PQpm4M"
   },
   "source": [
    "# Text generation using a RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2W6g03QsptwG"
   },
   "source": [
    "Given a sequence of words from this data, train a model to predict the next word in the sequence. Longer sequences of text can be generated by calling the model repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbd3E0IuHwjz"
   },
   "source": [
    "**Mount your Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Dikrw1ylHtAL"
   },
   "outputs": [],
   "source": [
    "#Mount drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnIX_mLXHdxS"
   },
   "source": [
    "### Import Keras and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "voJwnMdZjNPS"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0TmrQuvpHdxU"
   },
   "outputs": [],
   "source": [
    "#Already done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKBXQflGlPjG"
   },
   "source": [
    "## Download data\n",
    "Reference: Data is collected from http://www.gutenberg.org\n",
    "\n",
    "For the lab purpose, you can load the dataset provided by Great Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s321mV4DHdxZ"
   },
   "source": [
    "### Load the Oscar Wilde dataset\n",
    "\n",
    "Store all the \".txt\" file names in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VUanlzNJHdxa"
   },
   "outputs": [],
   "source": [
    "fileLst = os.listdir(project_path+'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "8c4ufjO4jONs",
    "outputId": "0a8f02fc-2127-4c0c-d728-8085c9526bcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Soul of Man.txt',\n",
       " 'The Canterville Ghost.txt',\n",
       " 'The Happy Prince and other tales.txt',\n",
       " 'Selected prose of oscar wilde with a Preface by Robert Ross.txt',\n",
       " 'The Duchess of Padua.txt',\n",
       " 'The Importance of Being Earnest.txt',\n",
       " 'The Picture of Dorian Gray.txt',\n",
       " 'Vera or, The Nihilists.txt',\n",
       " 'The Ballad of Reading Gaol.txt',\n",
       " 'Shorter Prose Pieces.txt']"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glr4hv6uZkL-"
   },
   "source": [
    "### Read the data\n",
    "\n",
    "Read contents of every file from the list and append the text in a new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zLrMMjrkRt9x"
   },
   "outputs": [],
   "source": [
    "os.chdir(project_path+'data')\n",
    "textLst = []\n",
    "for txtFile in fileLst:\n",
    "  f = open(txtFile, \"r\")\n",
    "  textLst.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "i1puLo5Rj_kV",
    "outputId": "67e6fff2-9bb6-4017-9ff8-a3dfa17b5d51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook, The Soul of Man, by Oscar Wilde\\n\\n\\nThis eBook is for the use of anyone anywhere in the United States and most\\nother parts of the world at no cost and with almost no restrictions \\nwhatsoever.  You may copy it, give it away or re-use it under the terms of\\nthe Project Gutenberg License included with this eBook or online at \\nwww.gutenberg.org.  If you are not located in the United States, you\\'ll have\\nto check the laws of the country where you are located before using this ebook.\\n\\n\\n\\n\\nTitle: The Soul of Man\\n\\n\\nAuthor: Oscar Wilde\\n\\n\\n\\nRelease Date: September 26, 2014  [eBook #1017]\\n[This file was first posted on August 10, 1997]\\n\\nLanguage: English\\n\\nCharacter set encoding: UTF-8\\n\\n\\n***START OF THE PROJECT GUTENBERG EBOOK THE SOUL OF MAN***\\n\\n\\nTranscribed from the 1909 Arthur L. Humphreys edition by David Price,\\nemail ccx074@pglaf.org\\n\\n                          [Picture: Book cover]\\n\\n\\n\\n\\nTHE\\nSOUL OF MAN\\n\\n\\n                                * * * * *\\n\\n                                  LONDON\\n                            ARTHUR L. HUMPREYS\\n                                   1900\\n\\n                                * * * * *\\n\\n                           _Second Impression_\\n\\n\\n\\n\\nTHE SOUL OF MAN\\n\\n\\nTHE chief advantage that would result from the establishment of Socialism\\nis, undoubtedly, the fact that Socialism would relieve us from that\\nsordid necessity of living for others which, in the present condition of\\nthings, presses so hardly upon almost everybody.  In fact, scarcely\\nanyone at all escapes.\\n\\nNow and then, in the course of the century, a great man of science, like\\nDarwin; a great poet, like Keats; a fine critical spirit, like M. Renan;\\na supreme artist, like Flaubert, has been able to isolate himself, to\\nkeep himself out of reach of the clamorous claims of others, to stand\\n‘under the shelter of the wall,’ as Plato puts it, and so to realise the\\nperfection of what was in him, to his own incomparable gain, and to the\\nincomparable and lasting gain of the whole world.  These, however, are\\nexceptions.  The majority of people spoil their lives by an unhealthy and\\nexaggerated altruism—are forced, indeed, so to spoil them.  They find\\nthemselves surrounded by hideous poverty, by hideous ugliness, by hideous\\nstarvation.  It is inevitable that they should be strongly moved by all\\nthis.  The emotions of man are stirred more quickly than man’s\\nintelligence; and, as I pointed out some time ago in an article on the\\nfunction of criticism, it is much more easy to have sympathy with\\nsuffering than it is to have sympathy with thought.  Accordingly, with\\nadmirable, though misdirected intentions, they very seriously and very\\nsentimentally set themselves to the task of remedying the evils that they\\nsee.  But their remedies do not cure the disease: they merely prolong it.\\nIndeed, their remedies are part of the disease.\\n\\nThey try to solve the problem of poverty, for instance, by keeping the\\npoor alive; or, in the case of a very advanced school, by amusing the\\npoor.\\n\\nBut this is not a solution: it is an aggravation of the difficulty.  The\\nproper aim is to try and reconstruct society on such a basis that poverty\\nwill be impossible.  And the altruistic virtues have really prevented the\\ncarrying out of this aim.  Just as the worst slave-owners were those who\\nwere kind to their slaves, and so prevented the horror of the system\\nbeing realised by those who suffered from it, and understood by those who\\ncontemplated it, so, in the present state of things in England, the\\npeople who do most harm are the people who try to do most good; and at\\nlast we have had the spectacle of men who have really studied the problem\\nand know the life—educated men who live in the East End—coming forward\\nand imploring the community to restrain its altruistic impulses of\\ncharity, benevolence, and the like.  They do so on the ground that such\\ncharity degrades and demoralises.  They are perfectly right.  Charity\\ncreates a multitude of sins.\\n\\nThere is also this to be said.  It is immoral to use private property in\\norder to alleviate the horrible evils that result from the institution of\\nprivate property.  It is both immoral and unfair.\\n\\nUnder Socialism all this will, of course, be altered.  There will be no\\npeople living in fetid dens and fetid rags, and bringing up unhealthy,\\nhunger-pinched children in the midst of impossible and absolutely\\nrepulsive surroundings.  The security of society will not depend, as it\\ndoes now, on the state of the weather.  If a frost comes we shall not\\nhave a hundred thousand men out of work, tramping about the streets in a\\nstate of disgusting misery, or whining to their neighbours for alms, or\\ncrowding round the doors of loathsome shelters to try and secure a hunch\\nof bread and a night’s unclean lodging.  Each member of the society will\\nshare in the general prosperity and happiness of the society, and if a\\nfrost comes no one will practically be anything the worse.\\n\\nUpon the other hand, Socialism itself will be of value simply because it\\nwill lead to Individualism.\\n\\nSocialism, Communism, or whatever one chooses to call it, by converting\\nprivate property into public wealth, and substituting co-operation for\\ncompetition, will restore society to its proper condition of a thoroughly\\nhealthy organism, and insure the material well-being of each member of\\nthe community.  It will, in fact, give Life its proper basis and its\\nproper environment.  But for the full development of Life to its highest\\nmode of perfection, something more is needed.  What is needed is\\nIndividualism.  If the Socialism is Authoritarian; if there are\\nGovernments armed with economic power as they are now with political\\npower; if, in a word, we are to have Industrial Tyrannies, then the last\\nstate of man will be worse than the first.  At present, in consequence of\\nthe existence of private property, a great many people are enabled to\\ndevelop a certain very limited amount of Individualism.  They are either\\nunder no necessity to work for their living, or are enabled to choose the\\nsphere of activity that is really congenial to them, and gives them\\npleasure.  These are the poets, the philosophers, the men of science, the\\nmen of culture—in a word, the real men, the men who have realised\\nthemselves, and in whom all Humanity gains a partial realisation.  Upon\\nthe other hand, there are a great many people who, having no private\\nproperty of their own, and being always on the brink of sheer starvation,\\nare compelled to do the work of beasts of burden, to do work that is\\nquite uncongenial to them, and to which they are forced by the\\nperemptory, unreasonable, degrading Tyranny of want.  These are the poor,\\nand amongst them there is no grace of manner, or charm of speech, or\\ncivilisation, or culture, or refinement in pleasures, or joy of life.\\nFrom their collective force Humanity gains much in material prosperity.\\nBut it is only the material result that it gains, and the man who is poor\\nis in himself absolutely of no importance.  He is merely the\\ninfinitesimal atom of a force that, so far from regarding him, crushes\\nhim: indeed, prefers him crushed, as in that case he is far more\\nobedient.\\n\\nOf course, it might be said that the Individualism generated under\\nconditions of private property is not always, or even as a rule, of a\\nfine or wonderful type, and that the poor, if they have not culture and\\ncharm, have still many virtues.  Both these statements would be quite\\ntrue.  The possession of private property is very often extremely\\ndemoralising, and that is, of course, one of the reasons why Socialism\\nwants to get rid of the institution.  In fact, property is really a\\nnuisance.  Some years ago people went about the country saying that\\nproperty has duties.  They said it so often and so tediously that, at\\nlast, the Church has begun to say it.  One hears it now from every\\npulpit.  It is perfectly true.  Property not merely has duties, but has\\nso many duties that its possession to any large extent is a bore.  It\\ninvolves endless claims upon one, endless attention to business, endless\\nbother.  If property had simply pleasures, we could stand it; but its\\nduties make it unbearable.  In the interest of the rich we must get rid\\nof it.  The virtues of the poor may be readily admitted, and are much to\\nbe regretted.  We are often told that the poor are grateful for charity.\\nSome of them are, no doubt, but the best amongst the poor are never\\ngrateful.  They are ungrateful, discontented, disobedient, and\\nrebellious.  They are quite right to be so.  Charity they feel to be a\\nridiculously inadequate mode of partial restitution, or a sentimental\\ndole, usually accompanied by some impertinent attempt on the part of the\\nsentimentalist to tyrannise over their private lives.  Why should they be\\ngrateful for the crumbs that fall from the rich man’s table?  They should\\nbe seated at the board, and are beginning to know it.  As for being\\ndiscontented, a man who would not be discontented with such surroundings\\nand such a low mode of life would be a perfect brute.  Disobedience, in\\nthe eyes of anyone who has read history, is man’s original virtue.  It is\\nthrough disobedience that progress has been made, through disobedience\\nand through rebellion.  Sometimes the poor are praised for being thrifty.\\nBut to recommend thrift to the poor is both grotesque and insulting.  It\\nis like advising a man who is starving to eat less.  For a town or\\ncountry labourer to practise thrift would be absolutely immoral.  Man\\nshould not be ready to show that he can live like a badly-fed animal.  He\\nshould decline to live like that, and should either steal or go on the\\nrates, which is considered by many to be a form of stealing.  As for\\nbegging, it is safer to beg than to take, but it is finer to take than to\\nbeg.  No: a poor man who is ungrateful, unthrifty, discontented, and\\nrebellious, is probably a real personality, and has much in him.  He is\\nat any rate a healthy protest.  As for the virtuous poor, one can pity\\nthem, of course, but one cannot possibly admire them.  They have made\\nprivate terms with the enemy, and sold their birthright for very bad\\npottage.  They must also be extraordinarily stupid.  I can quite\\nunderstand a man accepting laws that protect private property, and admit\\nof its accumulation, as long as he himself is able under those conditions\\nto realise some form of beautiful and intellectual life.  But it is\\nalmost incredible to me how a man whose life is marred and made hideous\\nby such laws can possibly acquiesce in their continuance.\\n\\nHowever, the explanation is not really difficult to find.  It is simply\\nthis.  Misery and poverty are so absolutely degrading, and exercise such\\na paralysing effect over the nature of men, that no class is ever really\\nconscious of its own suffering.  They have to be told of it by other\\npeople, and they often entirely disbelieve them.  What is said by great\\nemployers of labour against agitators is unquestionably true.  Agitators\\nare a set of interfering, meddling people, who come down to some\\nperfectly contented class of the community, and sow the seeds of\\ndiscontent amongst them.  That is the reason why agitators are so\\nabsolutely necessary.  Without them, in our incomplete state, there would\\nbe no advance towards civilisation.  Slavery was put down in America, not\\nin consequence of any action on the part of the slaves, or even any\\nexpress desire on their part that they should be free.  It was put down\\nentirely through the grossly illegal conduct of certain agitators in\\nBoston and elsewhere, who were not slaves themselves, nor owners of\\nslaves, nor had anything to do with the question really.  It was,\\nundoubtedly, the Abolitionists who set the torch alight, who began the\\nwhole thing.  And it is curious to note that from the slaves themselves\\nthey received, not merely very little assistance, but hardly any sympathy\\neven; and when at the close of the war the slaves found themselves free,\\nfound themselves indeed so absolutely free that they were free to starve,\\nmany of them bitterly regretted the new state of things.  To the thinker,\\nthe most tragic fact in the whole of the French Revolution is not that\\nMarie Antoinette was killed for being a queen, but that the starved\\npeasant of the Vendée voluntarily went out to die for the hideous cause\\nof feudalism.\\n\\nIt is clear, then, that no Authoritarian Socialism will do.  For while\\nunder the present system a very large number of people can lead lives of\\na certain amount of freedom and expression and happiness, under an\\nindustrial-barrack system, or a system of economic tyranny, nobody would\\nbe able to have any such freedom at all.  It is to be regretted that a\\nportion of our community should be practically in slavery, but to propose\\nto solve the problem by enslaving the entire community is childish.\\nEvery man must be left quite free to choose his own work.  No form of\\ncompulsion must be exercised over him.  If there is, his work will not be\\ngood for him, will not be good in itself, and will not be good for\\nothers.  And by work I simply mean activity of any kind.\\n\\nI hardly think that any Socialist, nowadays, would seriously propose that\\nan inspector should call every morning at each house to see that each\\ncitizen rose up and did manual labour for eight hours.  Humanity has got\\nbeyond that stage, and reserves such a form of life for the people whom,\\nin a very arbitrary manner, it chooses to call criminals.  But I confess\\nthat many of the socialistic views that I have come across seem to me to\\nbe tainted with ideas of authority, if not of actual compulsion.  Of\\ncourse, authority and compulsion are out of the question.  All\\nassociation must be quite voluntary.  It is only in voluntary\\nassociations that man is fine.\\n\\nBut it may be asked how Individualism, which is now more or less\\ndependent on the existence of private property for its development, will\\nbenefit by the abolition of such private property.  The answer is very\\nsimple.  It is true that, under existing conditions, a few men who have\\nhad private means of their own, such as Byron, Shelley, Browning, Victor\\nHugo, Baudelaire, and others, have been able to realise their personality\\nmore or less completely.  Not one of these men ever did a single day’s\\nwork for hire.  They were relieved from poverty.  They had an immense\\nadvantage.  The question is whether it would be for the good of\\nIndividualism that such an advantage should be taken away.  Let us\\nsuppose that it is taken away.  What happens then to Individualism?  How\\nwill it benefit?\\n\\nIt will benefit in this way.  Under the new conditions Individualism will\\nbe far freer, far finer, and far more intensified than it is now.  I am\\nnot talking of the great imaginatively-realised Individualism of such\\npoets as I have mentioned, but of the great actual Individualism latent\\nand potential in mankind generally.  For the recognition of private\\nproperty has really harmed Individualism, and obscured it, by confusing a\\nman with what he possesses.  It has led Individualism entirely astray.\\nIt has made gain not growth its aim.  So that man thought that the\\nimportant thing was to have, and did not know that the important thing is\\nto be.  The true perfection of man lies, not in what man has, but in what\\nman is.  Private property has crushed true Individualism, and set up an\\nIndividualism that is false.  It has debarred one part of the community\\nfrom being individual by starving them.  It has debarred the other part\\nof the community from being individual by putting them on the wrong road,\\nand encumbering them.  Indeed, so completely has man’s personality been\\nabsorbed by his possessions that the English law has always treated\\noffences against a man’s property with far more severity than offences\\nagainst his person, and property is still the test of complete\\ncitizenship.  The industry necessary for the making money is also very\\ndemoralising.  In a community like ours, where property confers immense\\ndistinction, social position, honour, respect, titles, and other pleasant\\nthings of the kind, man, being naturally ambitious, makes it his aim to\\naccumulate this property, and goes on wearily and tediously accumulating\\nit long after he has got far more than he wants, or can use, or enjoy, or\\nperhaps even know of.  Man will kill himself by overwork in order to\\nsecure property, and really, considering the enormous advantages that\\nproperty brings, one is hardly surprised.  One’s regret is that society\\nshould be constructed on such a basis that man has been forced into a\\ngroove in which he cannot freely develop what is wonderful, and\\nfascinating, and delightful in him—in which, in fact, he misses the true\\npleasure and joy of living.  He is also, under existing conditions, very\\ninsecure.  An enormously wealthy merchant may be—often is—at every moment\\nof his life at the mercy of things that are not under his control.  If\\nthe wind blows an extra point or so, or the weather suddenly changes, or\\nsome trivial thing happens, his ship may go down, his speculations may go\\nwrong, and he finds himself a poor man, with his social position quite\\ngone.  Now, nothing should be able to harm a man except himself.  Nothing\\nshould be able to rob a man at all.  What a man really has, is what is in\\nhim.  What is outside of him should be a matter of no importance.\\n\\nWith the abolition of private property, then, we shall have true,\\nbeautiful, healthy Individualism.  Nobody will waste his life in\\naccumulating things, and the symbols for things.  One will live.  To live\\nis the rarest thing in the world.  Most people exist, that is all.\\n\\nIt is a question whether we have ever seen the full expression of a\\npersonality, except on the imaginative plane of art.  In action, we never\\nhave.  Cæsar, says Mommsen, was the complete and perfect man.  But how\\ntragically insecure was Cæsar!  Wherever there is a man who exercises\\nauthority, there is a man who resists authority.  Cæsar was very perfect,\\nbut his perfection travelled by too dangerous a road.  Marcus Aurelius\\nwas the perfect man, says Renan.  Yes; the great emperor was a perfect\\nman.  But how intolerable were the endless claims upon him!  He staggered\\nunder the burden of the empire.  He was conscious how inadequate one man\\nwas to bear the weight of that Titan and too vast orb.  What I mean by a\\nperfect man is one who develops under perfect conditions; one who is not\\nwounded, or worried or maimed, or in danger.  Most personalities have\\nbeen obliged to be rebels.  Half their strength has been wasted in\\nfriction.  Byron’s personality, for instance, was terribly wasted in its\\nbattle with the stupidity, and hypocrisy, and Philistinism of the\\nEnglish.  Such battles do not always intensify strength: they often\\nexaggerate weakness.  Byron was never able to give us what he might have\\ngiven us.  Shelley escaped better.  Like Byron, he got out of England as\\nsoon as possible.  But he was not so well known.  If the English had had\\nany idea of what a great poet he really was, they would have fallen on\\nhim with tooth and nail, and made his life as unbearable to him as they\\npossibly could.  But he was not a remarkable figure in society, and\\nconsequently he escaped, to a certain degree.  Still, even in Shelley the\\nnote of rebellion is sometimes too strong.  The note of the perfect\\npersonality is not rebellion, but peace.\\n\\nIt will be a marvellous thing—the true personality of man—when we see it.\\nIt will grow naturally and simply, flowerlike, or as a tree grows.  It\\nwill not be at discord.  It will never argue or dispute.  It will not\\nprove things.  It will know everything.  And yet it will not busy itself\\nabout knowledge.  It will have wisdom.  Its value will not be measured by\\nmaterial things.  It will have nothing.  And yet it will have everything,\\nand whatever one takes from it, it will still have, so rich will it be.\\nIt will not be always meddling with others, or asking them to be like\\nitself.  It will love them because they will be different.  And yet while\\nit will not meddle with others, it will help all, as a beautiful thing\\nhelps us, by being what it is.  The personality of man will be very\\nwonderful.  It will be as wonderful as the personality of a child.\\n\\nIn its development it will be assisted by Christianity, if men desire\\nthat; but if men do not desire that, it will develop none the less\\nsurely.  For it will not worry itself about the past, nor care whether\\nthings happened or did not happen.  Nor will it admit any laws but its\\nown laws; nor any authority but its own authority.  Yet it will love\\nthose who sought to intensify it, and speak often of them.  And of these\\nChrist was one.\\n\\n‘Know thyself’ was written over the portal of the antique world.  Over\\nthe portal of the new world, ‘Be thyself’ shall be written.  And the\\nmessage of Christ to man was simply ‘Be thyself.’  That is the secret of\\nChrist.\\n\\nWhen Jesus talks about the poor he simply means personalities, just as\\nwhen he talks about the rich he simply means people who have not\\ndeveloped their personalities.  Jesus moved in a community that allowed\\nthe accumulation of private property just as ours does, and the gospel\\nthat he preached was not that in such a community it is an advantage for\\na man to live on scanty, unwholesome food, to wear ragged, unwholesome\\nclothes, to sleep in horrid, unwholesome dwellings, and a disadvantage\\nfor a man to live under healthy, pleasant, and decent conditions.  Such a\\nview would have been wrong there and then, and would, of course, be still\\nmore wrong now and in England; for as man moves northward the material\\nnecessities of life become of more vital importance, and our society is\\ninfinitely more complex, and displays far greater extremes of luxury and\\npauperism than any society of the antique world.  What Jesus meant, was\\nthis.  He said to man, ‘You have a wonderful personality.  Develop it.\\nBe yourself.  Don’t imagine that your perfection lies in accumulating or\\npossessing external things.  Your affection is inside of you.  If only\\nyou could realise that, you would not want to be rich.  Ordinary riches\\ncan be stolen from a man.  Real riches cannot.  In the treasury-house of\\nyour soul, there are infinitely precious things, that may not be taken\\nfrom you.  And so, try to so shape your life that external things will\\nnot harm you.  And try also to get rid of personal property.  It involves\\nsordid preoccupation, endless industry, continual wrong.  Personal\\nproperty hinders Individualism at every step.’  It is to be noted that\\nJesus never says that impoverished people are necessarily good, or\\nwealthy people necessarily bad.  That would not have been true.  Wealthy\\npeople are, as a class, better than impoverished people, more moral, more\\nintellectual, more well-behaved.  There is only one class in the\\ncommunity that thinks more about money than the rich, and that is the\\npoor.  The poor can think of nothing else.  That is the misery of being\\npoor.  What Jesus does say is that man reaches his perfection, not\\nthrough what he has, not even through what he does, but entirely through\\nwhat he is.  And so the wealthy young man who comes to Jesus is\\nrepresented as a thoroughly good citizen, who has broken none of the laws\\nof his state, none of the commandments of his religion.  He is quite\\nrespectable, in the ordinary sense of that extraordinary word.  Jesus\\nsays to him, ‘You should give up private property.  It hinders you from\\nrealising your perfection.  It is a drag upon you.  It is a burden.  Your\\npersonality does not need it.  It is within you, and not outside of you,\\nthat you will find what you really are, and what you really want.’  To\\nhis own friends he says the same thing.  He tells them to be themselves,\\nand not to be always worrying about other things.  What do other things\\nmatter?  Man is complete in himself.  When they go into the world, the\\nworld will disagree with them.  That is inevitable.  The world hates\\nIndividualism.  But that is not to trouble them.  They are to be calm and\\nself-centred.  If a man takes their cloak, they are to give him their\\ncoat, just to show that material things are of no importance.  If people\\nabuse them, they are not to answer back.  What does it signify?  The\\nthings people say of a man do not alter a man.  He is what he is.  Public\\nopinion is of no value whatsoever.  Even if people employ actual\\nviolence, they are not to be violent in turn.  That would be to fall to\\nthe same low level.  After all, even in prison, a man can be quite free.\\nHis soul can be free.  His personality can be untroubled.  He can be at\\npeace.  And, above all things, they are not to interfere with other\\npeople or judge them in any way.  Personality is a very mysterious thing.\\nA man cannot always be estimated by what he does.  He may keep the law,\\nand yet be worthless.  He may break the law, and yet be fine.  He may be\\nbad, without ever doing anything bad.  He may commit a sin against\\nsociety, and yet realise through that sin his true perfection.\\n\\nThere was a woman who was taken in adultery.  We are not told the history\\nof her love, but that love must have been very great; for Jesus said that\\nher sins were forgiven her, not because she repented, but because her\\nlove was so intense and wonderful.  Later on, a short time before his\\ndeath, as he sat at a feast, the woman came in and poured costly perfumes\\non his hair.  His friends tried to interfere with her, and said that it\\nwas an extravagance, and that the money that the perfume cost should have\\nbeen expended on charitable relief of people in want, or something of\\nthat kind.  Jesus did not accept that view.  He pointed out that the\\nmaterial needs of Man were great and very permanent, but that the\\nspiritual needs of Man were greater still, and that in one divine moment,\\nand by selecting its own mode of expression, a personality might make\\nitself perfect.  The world worships the woman, even now, as a saint.\\n\\nYes; there are suggestive things in Individualism.  Socialism annihilates\\nfamily life, for instance.  With the abolition of private property,\\nmarriage in its present form must disappear.  This is part of the\\nprogramme.  Individualism accepts this and makes it fine.  It converts\\nthe abolition of legal restraint into a form of freedom that will help\\nthe full development of personality, and make the love of man and woman\\nmore wonderful, more beautiful, and more ennobling.  Jesus knew this.  He\\nrejected the claims of family life, although they existed in his day and\\ncommunity in a very marked form.  ‘Who is my mother?  Who are my\\nbrothers?’ he said, when he was told that they wished to speak to him.\\nWhen one of his followers asked leave to go and bury his father, ‘Let the\\ndead bury the dead,’ was his terrible answer.  He would allow no claim\\nwhatsoever to be made on personality.\\n\\nAnd so he who would lead a Christlike life is he who is perfectly and\\nabsolutely himself.  He may be a great poet, or a great man of science;\\nor a young student at a University, or one who watches sheep upon a moor;\\nor a maker of dramas, like Shakespeare, or a thinker about God, like\\nSpinoza; or a child who plays in a garden, or a fisherman who throws his\\nnet into the sea.  It does not matter what he is, as long as he realises\\nthe perfection of the soul that is within him.  All imitation in morals\\nand in life is wrong.  Through the streets of Jerusalem at the present\\nday crawls one who is mad and carries a wooden cross on his shoulders.\\nHe is a symbol of the lives that are marred by imitation.  Father Damien\\nwas Christlike when he went out to live with the lepers, because in such\\nservice he realised fully what was best in him.  But he was not more\\nChristlike than Wagner when he realised his soul in music; or than\\nShelley, when he realised his soul in song.  There is no one type for\\nman.  There are as many perfections as there are imperfect men.  And\\nwhile to the claims of charity a man may yield and yet be free, to the\\nclaims of conformity no man may yield and remain free at all.\\n\\nIndividualism, then, is what through Socialism we are to attain to.  As a\\nnatural result the State must give up all idea of government.  It must\\ngive it up because, as a wise man once said many centuries before Christ,\\nthere is such a thing as leaving mankind alone; there is no such thing as\\ngoverning mankind.  All modes of government are failures.  Despotism is\\nunjust to everybody, including the despot, who was probably made for\\nbetter things.  Oligarchies are unjust to the many, and ochlocracies are\\nunjust to the few.  High hopes were once formed of democracy; but\\ndemocracy means simply the bludgeoning of the people by the people for\\nthe people.  It has been found out.  I must say that it was high time,\\nfor all authority is quite degrading.  It degrades those who exercise it,\\nand degrades those over whom it is exercised.  When it is violently,\\ngrossly, and cruelly used, it produces a good effect, by creating, or at\\nany rate bringing out, the spirit of revolt and Individualism that is to\\nkill it.  When it is used with a certain amount of kindness, and\\naccompanied by prizes and rewards, it is dreadfully demoralising.\\nPeople, in that case, are less conscious of the horrible pressure that is\\nbeing put on them, and so go through their lives in a sort of coarse\\ncomfort, like petted animals, without ever realising that they are\\nprobably thinking other people’s thoughts, living by other people’s\\nstandards, wearing practically what one may call other people’s\\nsecond-hand clothes, and never being themselves for a single moment.  ‘He\\nwho would be free,’ says a fine thinker, ‘must not conform.’  And\\nauthority, by bribing people to conform, produces a very gross kind of\\nover-fed barbarism amongst us.\\n\\nWith authority, punishment will pass away.  This will be a great gain—a\\ngain, in fact, of incalculable value.  As one reads history, not in the\\nexpurgated editions written for school-boys and passmen, but in the\\noriginal authorities of each time, one is absolutely sickened, not by the\\ncrimes that the wicked have committed, but by the punishments that the\\ngood have inflicted; and a community is infinitely more brutalised by the\\nhabitual employment of punishment, than it is by the occurrence of crime.\\nIt obviously follows that the more punishment is inflicted the more crime\\nis produced, and most modern legislation has clearly recognised this, and\\nhas made it its task to diminish punishment as far as it thinks it can.\\nWherever it has really diminished it, the results have always been\\nextremely good.  The less punishment, the less crime.  When there is no\\npunishment at all, crime will either cease to exist, or, if it occurs,\\nwill be treated by physicians as a very distressing form of dementia, to\\nbe cured by care and kindness.  For what are called criminals nowadays\\nare not criminals at all.  Starvation, and not sin, is the parent of\\nmodern crime.  That indeed is the reason why our criminals are, as a\\nclass, so absolutely uninteresting from any psychological point of view.\\nThey are not marvellous Macbeths and terrible Vautrins.  They are merely\\nwhat ordinary, respectable, commonplace people would be if they had not\\ngot enough to eat.  When private property is abolished there will be no\\nnecessity for crime, no demand for it; it will cease to exist.  Of\\ncourse, all crimes are not crimes against property, though such are the\\ncrimes that the English law, valuing what a man has more than what a man\\nis, punishes with the harshest and most horrible severity, if we except\\nthe crime of murder, and regard death as worse than penal servitude, a\\npoint on which our criminals, I believe, disagree.  But though a crime\\nmay not be against property, it may spring from the misery and rage and\\ndepression produced by our wrong system of property-holding, and so, when\\nthat system is abolished, will disappear.  When each member of the\\ncommunity has sufficient for his wants, and is not interfered with by his\\nneighbour, it will not be an object of any interest to him to interfere\\nwith anyone else.  Jealousy, which is an extraordinary source of crime in\\nmodern life, is an emotion closely bound up with our conceptions of\\nproperty, and under Socialism and Individualism will die out.  It is\\nremarkable that in communistic tribes jealousy is entirely unknown.\\n\\nNow as the State is not to govern, it may be asked what the State is to\\ndo.  The State is to be a voluntary association that will organise\\nlabour, and be the manufacturer and distributor of necessary commodities.\\nThe State is to make what is useful.  The individual is to make what is\\nbeautiful.  And as I have mentioned the word labour, I cannot help saying\\nthat a great deal of nonsense is being written and talked nowadays about\\nthe dignity of manual labour.  There is nothing necessarily dignified\\nabout manual labour at all, and most of it is absolutely degrading.  It\\nis mentally and morally injurious to man to do anything in which he does\\nnot find pleasure, and many forms of labour are quite pleasureless\\nactivities, and should be regarded as such.  To sweep a slushy crossing\\nfor eight hours, on a day when the east wind is blowing is a disgusting\\noccupation.  To sweep it with mental, moral, or physical dignity seems to\\nme to be impossible.  To sweep it with joy would be appalling.  Man is\\nmade for something better than disturbing dirt.  All work of that kind\\nshould be done by a machine.\\n\\nAnd I have no doubt that it will be so.  Up to the present, man has been,\\nto a certain extent, the slave of machinery, and there is something\\ntragic in the fact that as soon as man had invented a machine to do his\\nwork he began to starve.  This, however, is, of course, the result of our\\nproperty system and our system of competition.  One man owns a machine\\nwhich does the work of five hundred men.  Five hundred men are, in\\nconsequence, thrown out of employment, and, having no work to do, become\\nhungry and take to thieving.  The one man secures the produce of the\\nmachine and keeps it, and has five hundred times as much as he should\\nhave, and probably, which is of much more importance, a great deal more\\nthan he really wants.  Were that machine the property of all, every one\\nwould benefit by it.  It would be an immense advantage to the community.\\nAll unintellectual labour, all monotonous, dull labour, all labour that\\ndeals with dreadful things, and involves unpleasant conditions, must be\\ndone by machinery.  Machinery must work for us in coal mines, and do all\\nsanitary services, and be the stoker of steamers, and clean the streets,\\nand run messages on wet days, and do anything that is tedious or\\ndistressing.  At present machinery competes against man.  Under proper\\nconditions machinery will serve man.  There is no doubt at all that this\\nis the future of machinery, and just as trees grow while the country\\ngentleman is asleep, so while Humanity will be amusing itself, or\\nenjoying cultivated leisure—which, and not labour, is the aim of man—or\\nmaking beautiful things, or reading beautiful things, or simply\\ncontemplating the world with admiration and delight, machinery will be\\ndoing all the necessary and unpleasant work.  The fact is, that\\ncivilisation requires slaves.  The Greeks were quite right there.  Unless\\nthere are slaves to do the ugly, horrible, uninteresting work, culture\\nand contemplation become almost impossible.  Human slavery is wrong,\\ninsecure, and demoralising.  On mechanical slavery, on the slavery of the\\nmachine, the future of the world depends.  And when scientific men are no\\nlonger called upon to go down to a depressing East End and distribute bad\\ncocoa and worse blankets to starving people, they will have delightful\\nleisure in which to devise wonderful and marvellous things for their own\\njoy and the joy of everyone else.  There will be great storages of force\\nfor every city, and for every house if required, and this force man will\\nconvert into heat, light, or motion, according to his needs.  Is this\\nUtopian?  A map of the world that does not include Utopia is not worth\\neven glancing at, for it leaves out the one country at which Humanity is\\nalways landing.  And when Humanity lands there, it looks out, and, seeing\\na better country, sets sail.  Progress is the realisation of Utopias.\\n\\nNow, I have said that the community by means of organisation of machinery\\nwill supply the useful things, and that the beautiful things will be made\\nby the individual.  This is not merely necessary, but it is the only\\npossible way by which we can get either the one or the other.  An\\nindividual who has to make things for the use of others, and with\\nreference to their wants and their wishes, does not work with interest,\\nand consequently cannot put into his work what is best in him.  Upon the\\nother hand, whenever a community or a powerful section of a community, or\\na government of any kind, attempts to dictate to the artist what he is to\\ndo, Art either entirely vanishes, or becomes stereotyped, or degenerates\\ninto a low and ignoble form of craft.  A work of art is the unique result\\nof a unique temperament.  Its beauty comes from the fact that the author\\nis what he is.  It has nothing to do with the fact that other people want\\nwhat they want.  Indeed, the moment that an artist takes notice of what\\nother people want, and tries to supply the demand, he ceases to be an\\nartist, and becomes a dull or an amusing craftsman, an honest or a\\ndishonest tradesman.  He has no further claim to be considered as an\\nartist.  Art is the most intense mode of Individualism that the world has\\nknown.  I am inclined to say that it is the only real mode of\\nIndividualism that the world has known.  Crime, which, under certain\\nconditions, may seem to have created Individualism, must take cognisance\\nof other people and interfere with them.  It belongs to the sphere of\\naction.  But alone, without any reference to his neighbours, without any\\ninterference, the artist can fashion a beautiful thing; and if he does\\nnot do it solely for his own pleasure, he is not an artist at all.\\n\\nAnd it is to be noted that it is the fact that Art is this intense form\\nof Individualism that makes the public try to exercise over it in an\\nauthority that is as immoral as it is ridiculous, and as corrupting as it\\nis contemptible.  It is not quite their fault.  The public has always,\\nand in every age, been badly brought up.  They are continually asking Art\\nto be popular, to please their want of taste, to flatter their absurd\\nvanity, to tell them what they have been told before, to show them what\\nthey ought to be tired of seeing, to amuse them when they feel heavy\\nafter eating too much, and to distract their thoughts when they are\\nwearied of their own stupidity.  Now Art should never try to be popular.\\nThe public should try to make itself artistic.  There is a very wide\\ndifference.  If a man of science were told that the results of his\\nexperiments, and the conclusions that he arrived at, should be of such a\\ncharacter that they would not upset the received popular notions on the\\nsubject, or disturb popular prejudice, or hurt the sensibilities of\\npeople who knew nothing about science; if a philosopher were told that he\\nhad a perfect right to speculate in the highest spheres of thought,\\nprovided that he arrived at the same conclusions as were held by those\\nwho had never thought in any sphere at all—well, nowadays the man of\\nscience and the philosopher would be considerably amused.  Yet it is\\nreally a very few years since both philosophy and science were subjected\\nto brutal popular control, to authority in fact—the authority of either\\nthe general ignorance of the community, or the terror and greed for power\\nof an ecclesiastical or governmental class.  Of course, we have to a very\\ngreat extent got rid of any attempt on the part of the community, or the\\nChurch, or the Government, to interfere with the individualism of\\nspeculative thought, but the attempt to interfere with the individualism\\nof imaginative art still lingers.  In fact, it does more than linger; it\\nis aggressive, offensive, and brutalising.\\n\\nIn England, the arts that have escaped best are the arts in which the\\npublic take no interest.  Poetry is an instance of what I mean.  We have\\nbeen able to have fine poetry in England because the public do not read\\nit, and consequently do not influence it.  The public like to insult\\npoets because they are individual, but once they have insulted them, they\\nleave them alone.  In the case of the novel and the drama, arts in which\\nthe public do take an interest, the result of the exercise of popular\\nauthority has been absolutely ridiculous.  No country produces such\\nbadly-written fiction, such tedious, common work in the novel form, such\\nsilly, vulgar plays as England.  It must necessarily be so.  The popular\\nstandard is of such a character that no artist can get to it.  It is at\\nonce too easy and too difficult to be a popular novelist.  It is too\\neasy, because the requirements of the public as far as plot, style,\\npsychology, treatment of life, and treatment of literature are concerned\\nare within the reach of the very meanest capacity and the most\\nuncultivated mind.  It is too difficult, because to meet such\\nrequirements the artist would have to do violence to his temperament,\\nwould have to write not for the artistic joy of writing, but for the\\namusement of half-educated people, and so would have to suppress his\\nindividualism, forget his culture, annihilate his style, and surrender\\neverything that is valuable in him.  In the case of the drama, things are\\na little better: the theatre-going public like the obvious, it is true,\\nbut they do not like the tedious; and burlesque and farcical comedy, the\\ntwo most popular forms, are distinct forms of art.  Delightful work may\\nbe produced under burlesque and farcical conditions, and in work of this\\nkind the artist in England is allowed very great freedom.  It is when one\\ncomes to the higher forms of the drama that the result of popular control\\nis seen.  The one thing that the public dislike is novelty.  Any attempt\\nto extend the subject-matter of art is extremely distasteful to the\\npublic; and yet the vitality and progress of art depend in a large\\nmeasure on the continual extension of subject-matter.  The public dislike\\nnovelty because they are afraid of it.  It represents to them a mode of\\nIndividualism, an assertion on the part of the artist that he selects his\\nown subject, and treats it as he chooses.  The public are quite right in\\ntheir attitude.  Art is Individualism, and Individualism is a disturbing\\nand disintegrating force.  Therein lies its immense value.  For what it\\nseeks to disturb is monotony of type, slavery of custom, tyranny of\\nhabit, and the reduction of man to the level of a machine.  In Art, the\\npublic accept what has been, because they cannot alter it, not because\\nthey appreciate it.  They swallow their classics whole, and never taste\\nthem.  They endure them as the inevitable, and as they cannot mar them,\\nthey mouth about them.  Strangely enough, or not strangely, according to\\none’s own views, this acceptance of the classics does a great deal of\\nharm.  The uncritical admiration of the Bible and Shakespeare in England\\nis an instance of what I mean.  With regard to the Bible, considerations\\nof ecclesiastical authority enter into the matter, so that I need not\\ndwell upon the point.\\n\\nBut in the case of Shakespeare it is quite obvious that the public really\\nsee neither the beauties nor the defects of his plays.  If they saw the\\nbeauties, they would not object to the development of the drama; and if\\nthey saw the defects, they would not object to the development of the\\ndrama either.  The fact is, the public make use of the classics of a\\ncountry as a means of checking the progress of Art.  They degrade the\\nclassics into authorities.  They use them as bludgeons for preventing the\\nfree expression of Beauty in new forms.  They are always asking a writer\\nwhy he does not write like somebody else, or a painter why he does not\\npaint like somebody else, quite oblivious of the fact that if either of\\nthem did anything of the kind he would cease to be an artist.  A fresh\\nmode of Beauty is absolutely distasteful to them, and whenever it appears\\nthey get so angry, and bewildered that they always use two stupid\\nexpressions—one is that the work of art is grossly unintelligible; the\\nother, that the work of art is grossly immoral.  What they mean by these\\nwords seems to me to be this.  When they say a work is grossly\\nunintelligible, they mean that the artist has said or made a beautiful\\nthing that is new; when they describe a work as grossly immoral, they\\nmean that the artist has said or made a beautiful thing that is true.\\nThe former expression has reference to style; the latter to\\nsubject-matter.  But they probably use the words very vaguely, as an\\nordinary mob will use ready-made paving-stones.  There is not a single\\nreal poet or prose-writer of this century, for instance, on whom the\\nBritish public have not solemnly conferred diplomas of immorality, and\\nthese diplomas practically take the place, with us, of what in France, is\\nthe formal recognition of an Academy of Letters, and fortunately make the\\nestablishment of such an institution quite unnecessary in England.  Of\\ncourse, the public are very reckless in their use of the word.  That they\\nshould have called Wordsworth an immoral poet, was only to be expected.\\nWordsworth was a poet.  But that they should have called Charles Kingsley\\nan immoral novelist is extraordinary.  Kingsley’s prose was not of a very\\nfine quality.  Still, there is the word, and they use it as best they\\ncan.  An artist is, of course, not disturbed by it.  The true artist is a\\nman who believes absolutely in himself, because he is absolutely himself.\\nBut I can fancy that if an artist produced a work of art in England that\\nimmediately on its appearance was recognised by the public, through their\\nmedium, which is the public press, as a work that was quite intelligible\\nand highly moral, he would begin to seriously question whether in its\\ncreation he had really been himself at all, and consequently whether the\\nwork was not quite unworthy of him, and either of a thoroughly\\nsecond-rate order, or of no artistic value whatsoever.\\n\\nPerhaps, however, I have wronged the public in limiting them to such\\nwords as ‘immoral,’ ‘unintelligible,’ ‘exotic,’ and ‘unhealthy.’  There\\nis one other word that they use.  That word is ‘morbid.’  They do not use\\nit often.  The meaning of the word is so simple that they are afraid of\\nusing it.  Still, they use it sometimes, and, now and then, one comes\\nacross it in popular newspapers.  It is, of course, a ridiculous word to\\napply to a work of art.  For what is morbidity but a mood of emotion or a\\nmode of thought that one cannot express?  The public are all morbid,\\nbecause the public can never find expression for anything.  The artist is\\nnever morbid.  He expresses everything.  He stands outside his subject,\\nand through its medium produces incomparable and artistic effects.  To\\ncall an artist morbid because he deals with morbidity as his\\nsubject-matter is as silly as if one called Shakespeare mad because he\\nwrote ‘King Lear.’\\n\\nOn the whole, an artist in England gains something by being attacked.\\nHis individuality is intensified.  He becomes more completely himself.\\nOf course, the attacks are very gross, very impertinent, and very\\ncontemptible.  But then no artist expects grace from the vulgar mind, or\\nstyle from the suburban intellect.  Vulgarity and stupidity are two very\\nvivid facts in modern life.  One regrets them, naturally.  But there they\\nare.  They are subjects for study, like everything else.  And it is only\\nfair to state, with regard to modern journalists, that they always\\napologise to one in private for what they have written against one in\\npublic.\\n\\nWithin the last few years two other adjectives, it may be mentioned, have\\nbeen added to the very limited vocabulary of art-abuse that is at the\\ndisposal of the public.  One is the word ‘unhealthy,’ the other is the\\nword ‘exotic.’  The latter merely expresses the rage of the momentary\\nmushroom against the immortal, entrancing, and exquisitely lovely orchid.\\nIt is a tribute, but a tribute of no importance.  The word ‘unhealthy,’\\nhowever, admits of analysis.  It is a rather interesting word.  In fact,\\nit is so interesting that the people who use it do not know what it\\nmeans.\\n\\nWhat does it mean?  What is a healthy, or an unhealthy work of art?  All\\nterms that one applies to a work of art, provided that one applies them\\nrationally, have reference to either its style or its subject, or to both\\ntogether.  From the point of view of style, a healthy work of art is one\\nwhose style recognises the beauty of the material it employs, be that\\nmaterial one of words or of bronze, of colour or of ivory, and uses that\\nbeauty as a factor in producing the æsthetic effect.  From the point of\\nview of subject, a healthy work of art is one the choice of whose subject\\nis conditioned by the temperament of the artist, and comes directly out\\nof it.  In fine, a healthy work of art is one that has both perfection\\nand personality.  Of course, form and substance cannot be separated in a\\nwork of art; they are always one.  But for purposes of analysis, and\\nsetting the wholeness of æsthetic impression aside for a moment, we can\\nintellectually so separate them.  An unhealthy work of art, on the other\\nhand, is a work whose style is obvious, old-fashioned, and common, and\\nwhose subject is deliberately chosen, not because the artist has any\\npleasure in it, but because he thinks that the public will pay him for\\nit.  In fact, the popular novel that the public calls healthy is always a\\nthoroughly unhealthy production; and what the public call an unhealthy\\nnovel is always a beautiful and healthy work of art.\\n\\nI need hardly say that I am not, for a single moment, complaining that\\nthe public and the public press misuse these words.  I do not see how,\\nwith their lack of comprehension of what Art is, they could possibly use\\nthem in the proper sense.  I am merely pointing out the misuse; and as\\nfor the origin of the misuse and the meaning that lies behind it all, the\\nexplanation is very simple.  It comes from the barbarous conception of\\nauthority.  It comes from the natural inability of a community corrupted\\nby authority to understand or appreciate Individualism.  In a word, it\\ncomes from that monstrous and ignorant thing that is called Public\\nOpinion, which, bad and well-meaning as it is when it tries to control\\naction, is infamous and of evil meaning when it tries to control Thought\\nor Art.\\n\\nIndeed, there is much more to be said in favour of the physical force of\\nthe public than there is in favour of the public’s opinion.  The former\\nmay be fine.  The latter must be foolish.  It is often said that force is\\nno argument.  That, however, entirely depends on what one wants to prove.\\nMany of the most important problems of the last few centuries, such as\\nthe continuance of personal government in England, or of feudalism in\\nFrance, have been solved entirely by means of physical force.  The very\\nviolence of a revolution may make the public grand and splendid for a\\nmoment.  It was a fatal day when the public discovered that the pen is\\nmightier than the paving-stone, and can be made as offensive as the\\nbrickbat.  They at once sought for the journalist, found him, developed\\nhim, and made him their industrious and well-paid servant.  It is greatly\\nto be regretted, for both their sakes.  Behind the barricade there may be\\nmuch that is noble and heroic.  But what is there behind the\\nleading-article but prejudice, stupidity, cant, and twaddle?  And when\\nthese four are joined together they make a terrible force, and constitute\\nthe new authority.\\n\\nIn old days men had the rack.  Now they have the press.  That is an\\nimprovement certainly.  But still it is very bad, and wrong, and\\ndemoralising.  Somebody—was it Burke?—called journalism the fourth\\nestate.  That was true at the time, no doubt.  But at the present moment\\nit really is the only estate.  It has eaten up the other three.  The\\nLords Temporal say nothing, the Lords Spiritual have nothing to say, and\\nthe House of Commons has nothing to say and says it.  We are dominated by\\nJournalism.  In America the President reigns for four years, and\\nJournalism governs for ever and ever.  Fortunately in America Journalism\\nhas carried its authority to the grossest and most brutal extreme.  As a\\nnatural consequence it has begun to create a spirit of revolt.  People\\nare amused by it, or disgusted by it, according to their temperaments.\\nBut it is no longer the real force it was.  It is not seriously treated.\\nIn England, Journalism, not, except in a few well-known instances, having\\nbeen carried to such excesses of brutality, is still a great factor, a\\nreally remarkable power.  The tyranny that it proposes to exercise over\\npeople’s private lives seems to me to be quite extraordinary.  The fact\\nis, that the public have an insatiable curiosity to know everything,\\nexcept what is worth knowing.  Journalism, conscious of this, and having\\ntradesman-like habits, supplies their demands.  In centuries before ours\\nthe public nailed the ears of journalists to the pump.  That was quite\\nhideous.  In this century journalists have nailed their own ears to the\\nkeyhole.  That is much worse.  And what aggravates the mischief is that\\nthe journalists who are most to blame are not the amusing journalists who\\nwrite for what are called Society papers.  The harm is done by the\\nserious, thoughtful, earnest journalists, who solemnly, as they are doing\\nat present, will drag before the eyes of the public some incident in the\\nprivate life of a great statesman, of a man who is a leader of political\\nthought as he is a creator of political force, and invite the public to\\ndiscuss the incident, to exercise authority in the matter, to give their\\nviews, and not merely to give their views, but to carry them into action,\\nto dictate to the man upon all other points, to dictate to his party, to\\ndictate to his country; in fact, to make themselves ridiculous,\\noffensive, and harmful.  The private lives of men and women should not be\\ntold to the public.  The public have nothing to do with them at all.  In\\nFrance they manage these things better.  There they do not allow the\\ndetails of the trials that take place in the divorce courts to be\\npublished for the amusement or criticism of the public.  All that the\\npublic are allowed to know is that the divorce has taken place and was\\ngranted on petition of one or other or both of the married parties\\nconcerned.  In France, in fact, they limit the journalist, and allow the\\nartist almost perfect freedom.  Here we allow absolute freedom to the\\njournalist, and entirely limit the artist.  English public opinion, that\\nis to say, tries to constrain and impede and warp the man who makes\\nthings that are beautiful in effect, and compels the journalist to retail\\nthings that are ugly, or disgusting, or revolting in fact, so that we\\nhave the most serious journalists in the world, and the most indecent\\nnewspapers.  It is no exaggeration to talk of compulsion.  There are\\npossibly some journalists who take a real pleasure in publishing horrible\\nthings, or who, being poor, look to scandals as forming a sort of\\npermanent basis for an income.  But there are other journalists, I feel\\ncertain, men of education and cultivation, who really dislike publishing\\nthese things, who know that it is wrong to do so, and only do it because\\nthe unhealthy conditions under which their occupation is carried on\\noblige them to supply the public with what the public wants, and to\\ncompete with other journalists in making that supply as full and\\nsatisfying to the gross popular appetite as possible.  It is a very\\ndegrading position for any body of educated men to be placed in, and I\\nhave no doubt that most of them feel it acutely.\\n\\nHowever, let us leave what is really a very sordid side of the subject,\\nand return to the question of popular control in the matter of Art, by\\nwhich I mean Public Opinion dictating to the artist the form which he is\\nto use, the mode in which he is to use it, and the materials with which\\nhe is to work.  I have pointed out that the arts which have escaped best\\nin England are the arts in which the public have not been interested.\\nThey are, however, interested in the drama, and as a certain advance has\\nbeen made in the drama within the last ten or fifteen years, it is\\nimportant to point out that this advance is entirely due to a few\\nindividual artists refusing to accept the popular want of taste as their\\nstandard, and refusing to regard Art as a mere matter of demand and\\nsupply.  With his marvellous and vivid personality, with a style that has\\nreally a true colour-element in it, with his extraordinary power, not\\nover mere mimicry but over imaginative and intellectual creation, Mr\\nIrving, had his sole object been to give the public what they wanted,\\ncould have produced the commonest plays in the commonest manner, and made\\nas much success and money as a man could possibly desire.  But his object\\nwas not that.  His object was to realise his own perfection as an artist,\\nunder certain conditions, and in certain forms of Art.  At first he\\nappealed to the few: now he has educated the many.  He has created in the\\npublic both taste and temperament.  The public appreciate his artistic\\nsuccess immensely.  I often wonder, however, whether the public\\nunderstand that that success is entirely due to the fact that he did not\\naccept their standard, but realised his own.  With their standard the\\nLyceum would have been a sort of second-rate booth, as some of the\\npopular theatres in London are at present.  Whether they understand it or\\nnot the fact however remains, that taste and temperament have, to a\\ncertain extent been created in the public, and that the public is capable\\nof developing these qualities.  The problem then is, why do not the\\npublic become more civilised?  They have the capacity.  What stops them?\\n\\nThe thing that stops them, it must be said again, is their desire to\\nexercise authority over the artist and over works of art.  To certain\\ntheatres, such as the Lyceum and the Haymarket, the public seem to come\\nin a proper mood.  In both of these theatres there have been individual\\nartists, who have succeeded in creating in their audiences—and every\\ntheatre in London has its own audience—the temperament to which Art\\nappeals.  And what is that temperament?  It is the temperament of\\nreceptivity.  That is all.\\n\\nIf a man approaches a work of art with any desire to exercise authority\\nover it and the artist, he approaches it in such a spirit that he cannot\\nreceive any artistic impression from it at all.  The work of art is to\\ndominate the spectator: the spectator is not to dominate the work of art.\\nThe spectator is to be receptive.  He is to be the violin on which the\\nmaster is to play.  And the more completely he can suppress his own silly\\nviews, his own foolish prejudices, his own absurd ideas of what Art\\nshould be, or should not be, the more likely he is to understand and\\nappreciate the work of art in question.  This is, of course, quite\\nobvious in the case of the vulgar theatre-going public of English men and\\nwomen.  But it is equally true of what are called educated people.  For\\nan educated person’s ideas of Art are drawn naturally from what Art has\\nbeen, whereas the new work of art is beautiful by being what Art has\\nnever been; and to measure it by the standard of the past is to measure\\nit by a standard on the rejection of which its real perfection depends.\\nA temperament capable of receiving, through an imaginative medium, and\\nunder imaginative conditions, new and beautiful impressions, is the only\\ntemperament that can appreciate a work of art.  And true as this is in\\nthe case of the appreciation of sculpture and painting, it is still more\\ntrue of the appreciation of such arts as the drama.  For a picture and a\\nstatue are not at war with Time.  They take no count of its succession.\\nIn one moment their unity may be apprehended.  In the case of literature\\nit is different.  Time must be traversed before the unity of effect is\\nrealised.  And so, in the drama, there may occur in the first act of the\\nplay something whose real artistic value may not be evident to the\\nspectator till the third or fourth act is reached.  Is the silly fellow\\nto get angry and call out, and disturb the play, and annoy the artists?\\nNo.  The honest man is to sit quietly, and know the delightful emotions\\nof wonder, curiosity, and suspense.  He is not to go to the play to lose\\na vulgar temper.  He is to go to the play to realise an artistic\\ntemperament.  He is to go to the play to gain an artistic temperament.\\nHe is not the arbiter of the work of art.  He is one who is admitted to\\ncontemplate the work of art, and, if the work be fine, to forget in its\\ncontemplation and the egotism that mars him—the egotism of his ignorance,\\nor the egotism of his information.  This point about the drama is hardly,\\nI think, sufficiently recognised.  I can quite understand that were\\n‘Macbeth’ produced for the first time before a modern London audience,\\nmany of the people present would strongly and vigorously object to the\\nintroduction of the witches in the first act, with their grotesque\\nphrases and their ridiculous words.  But when the play is over one\\nrealises that the laughter of the witches in ‘Macbeth’ is as terrible as\\nthe laughter of madness in ‘Lear,’ more terrible than the laughter of\\nIago in the tragedy of the Moor.  No spectator of art needs a more\\nperfect mood of receptivity than the spectator of a play.  The moment he\\nseeks to exercise authority he becomes the avowed enemy of Art and of\\nhimself.  Art does not mind.  It is he who suffers.\\n\\nWith the novel it is the same thing.  Popular authority and the\\nrecognition of popular authority are fatal.  Thackeray’s ‘Esmond’ is a\\nbeautiful work of art because he wrote it to please himself.  In his\\nother novels, in ‘Pendennis,’ in ‘Philip,’ in ‘Vanity Fair’ even, at\\ntimes, he is too conscious of the public, and spoils his work by\\nappealing directly to the sympathies of the public, or by directly\\nmocking at them.  A true artist takes no notice whatever of the public.\\nThe public are to him non-existent.  He has no poppied or honeyed cakes\\nthrough which to give the monster sleep or sustenance.  He leaves that to\\nthe popular novelist.  One incomparable novelist we have now in England,\\nMr George Meredith.  There are better artists in France, but France has\\nno one whose view of life is so large, so varied, so imaginatively true.\\nThere are tellers of stories in Russia who have a more vivid sense of\\nwhat pain in fiction may be.  But to him belongs philosophy in fiction.\\nHis people not merely live, but they live in thought.  One can see them\\nfrom myriad points of view.  They are suggestive.  There is soul in them\\nand around them.  They are interpretative and symbolic.  And he who made\\nthem, those wonderful quickly-moving figures, made them for his own\\npleasure, and has never asked the public what they wanted, has never\\ncared to know what they wanted, has never allowed the public to dictate\\nto him or influence him in any way but has gone on intensifying his own\\npersonality, and producing his own individual work.  At first none came\\nto him.  That did not matter.  Then the few came to him.  That did not\\nchange him.  The many have come now.  He is still the same.  He is an\\nincomparable novelist.\\n\\nWith the decorative arts it is not different.  The public clung with\\nreally pathetic tenacity to what I believe were the direct traditions of\\nthe Great Exhibition of international vulgarity, traditions that were so\\nappalling that the houses in which people lived were only fit for blind\\npeople to live in.  Beautiful things began to be made, beautiful colours\\ncame from the dyer’s hand, beautiful patterns from the artist’s brain,\\nand the use of beautiful things and their value and importance were set\\nforth.  The public were really very indignant.  They lost their temper.\\nThey said silly things.  No one minded.  No one was a whit the worse.  No\\none accepted the authority of public opinion.  And now it is almost\\nimpossible to enter any modern house without seeing some recognition of\\ngood taste, some recognition of the value of lovely surroundings, some\\nsign of appreciation of beauty.  In fact, people’s houses are, as a rule,\\nquite charming nowadays.  People have been to a very great extent\\ncivilised.  It is only fair to state, however, that the extraordinary\\nsuccess of the revolution in house-decoration and furniture and the like\\nhas not really been due to the majority of the public developing a very\\nfine taste in such matters.  It has been chiefly due to the fact that the\\ncraftsmen of things so appreciated the pleasure of making what was\\nbeautiful, and woke to such a vivid consciousness of the hideousness and\\nvulgarity of what the public had previously wanted, that they simply\\nstarved the public out.  It would be quite impossible at the present\\nmoment to furnish a room as rooms were furnished a few years ago, without\\ngoing for everything to an auction of second-hand furniture from some\\nthird-rate lodging-house.  The things are no longer made.  However they\\nmay object to it, people must nowadays have something charming in their\\nsurroundings.  Fortunately for them, their assumption of authority in\\nthese art-matters came to entire grief.\\n\\nIt is evident, then, that all authority in such things is bad.  People\\nsometimes inquire what form of government is most suitable for an artist\\nto live under.  To this question there is only one answer.  The form of\\ngovernment that is most suitable to the artist is no government at all.\\nAuthority over him and his art is ridiculous.  It has been stated that\\nunder despotisms artists have produced lovely work.  This is not quite\\nso.  Artists have visited despots, not as subjects to be tyrannised over,\\nbut as wandering wonder-makers, as fascinating vagrant personalities, to\\nbe entertained and charmed and suffered to be at peace, and allowed to\\ncreate.  There is this to be said in favour of the despot, that he, being\\nan individual, may have culture, while the mob, being a monster, has\\nnone.  One who is an Emperor and King may stoop down to pick up a brush\\nfor a painter, but when the democracy stoops down it is merely to throw\\nmud.  And yet the democracy have not so far to stoop as the emperor.  In\\nfact, when they want to throw mud they have not to stoop at all.  But\\nthere is no necessity to separate the monarch from the mob; all authority\\nis equally bad.\\n\\nThere are three kinds of despots.  There is the despot who tyrannises\\nover the body.  There is the despot who tyrannises over the soul.  There\\nis the despot who tyrannises over the soul and body alike.  The first is\\ncalled the Prince.  The second is called the Pope.  The third is called\\nthe People.  The Prince may be cultivated.  Many Princes have been.  Yet\\nin the Prince there is danger.  One thinks of Dante at the bitter feast\\nin Verona, of Tasso in Ferrara’s madman’s cell.  It is better for the\\nartist not to live with Princes.  The Pope may be cultivated.  Many Popes\\nhave been; the bad Popes have been.  The bad Popes loved Beauty, almost\\nas passionately, nay, with as much passion as the good Popes hated\\nThought.  To the wickedness of the Papacy humanity owes much.  The\\ngoodness of the Papacy owes a terrible debt to humanity.  Yet, though the\\nVatican has kept the rhetoric of its thunders, and lost the rod of its\\nlightning, it is better for the artist not to live with Popes.  It was a\\nPope who said of Cellini to a conclave of Cardinals that common laws and\\ncommon authority were not made for men such as he; but it was a Pope who\\nthrust Cellini into prison, and kept him there till he sickened with\\nrage, and created unreal visions for himself, and saw the gilded sun\\nenter his room, and grew so enamoured of it that he sought to escape, and\\ncrept out from tower to tower, and falling through dizzy air at dawn,\\nmaimed himself, and was by a vine-dresser covered with vine leaves, and\\ncarried in a cart to one who, loving beautiful things, had care of him.\\nThere is danger in Popes.  And as for the People, what of them and their\\nauthority?  Perhaps of them and their authority one has spoken enough.\\nTheir authority is a thing blind, deaf, hideous, grotesque, tragic,\\namusing, serious, and obscene.  It is impossible for the artist to live\\nwith the People.  All despots bribe.  The people bribe and brutalise.\\nWho told them to exercise authority?  They were made to live, to listen,\\nand to love.  Someone has done them a great wrong.  They have marred\\nthemselves by imitation of their inferiors.  They have taken the sceptre\\nof the Prince.  How should they use it?  They have taken the triple tiara\\nof the Pope.  How should they carry its burden?  They are as a clown\\nwhose heart is broken.  They are as a priest whose soul is not yet born.\\nLet all who love Beauty pity them.  Though they themselves love not\\nBeauty, yet let them pity themselves.  Who taught them the trick of\\ntyranny?\\n\\nThere are many other things that one might point out.  One might point\\nout how the Renaissance was great, because it sought to solve no social\\nproblem, and busied itself not about such things, but suffered the\\nindividual to develop freely, beautifully, and naturally, and so had\\ngreat and individual artists, and great and individual men.  One might\\npoint out how Louis XIV., by creating the modern state, destroyed the\\nindividualism of the artist, and made things monstrous in their monotony\\nof repetition, and contemptible in their conformity to rule, and\\ndestroyed throughout all France all those fine freedoms of expression\\nthat had made tradition new in beauty, and new modes one with antique\\nform.  But the past is of no importance.  The present is of no\\nimportance.  It is with the future that we have to deal.  For the past is\\nwhat man should not have been.  The present is what man ought not to be.\\nThe future is what artists are.\\n\\nIt will, of course, be said that such a scheme as is set forth here is\\nquite unpractical, and goes against human nature.  This is perfectly\\ntrue.  It is unpractical, and it goes against human nature.  This is why\\nit is worth carrying out, and that is why one proposes it.  For what is a\\npractical scheme?  A practical scheme is either a scheme that is already\\nin existence, or a scheme that could be carried out under existing\\nconditions.  But it is exactly the existing conditions that one objects\\nto; and any scheme that could accept these conditions is wrong and\\nfoolish.  The conditions will be done away with, and human nature will\\nchange.  The only thing that one really knows about human nature is that\\nit changes.  Change is the one quality we can predicate of it.  The\\nsystems that fail are those that rely on the permanency of human nature,\\nand not on its growth and development.  The error of Louis XIV. was that\\nhe thought human nature would always be the same.  The result of his\\nerror was the French Revolution.  It was an admirable result.  All the\\nresults of the mistakes of governments are quite admirable.\\n\\nIt is to be noted also that Individualism does not come to man with any\\nsickly cant about duty, which merely means doing what other people want\\nbecause they want it; or any hideous cant about self-sacrifice, which is\\nmerely a survival of savage mutilation.  In fact, it does not come to man\\nwith any claims upon him at all.  It comes naturally and inevitably out\\nof man.  It is the point to which all development tends.  It is the\\ndifferentiation to which all organisms grow.  It is the perfection that\\nis inherent in every mode of life, and towards which every mode of life\\nquickens.  And so Individualism exercises no compulsion over man.  On the\\ncontrary, it says to man that he should suffer no compulsion to be\\nexercised over him.  It does not try to force people to be good.  It\\nknows that people are good when they are let alone.  Man will develop\\nIndividualism out of himself.  Man is now so developing Individualism.\\nTo ask whether Individualism is practical is like asking whether\\nEvolution is practical.  Evolution is the law of life, and there is no\\nevolution except towards Individualism.  Where this tendency is not\\nexpressed, it is a case of artificially-arrested growth, or of disease,\\nor of death.\\n\\nIndividualism will also be unselfish and unaffected.  It has been pointed\\nout that one of the results of the extraordinary tyranny of authority is\\nthat words are absolutely distorted from their proper and simple meaning,\\nand are used to express the obverse of their right signification.  What\\nis true about Art is true about Life.  A man is called affected,\\nnowadays, if he dresses as he likes to dress.  But in doing that he is\\nacting in a perfectly natural manner.  Affectation, in such matters,\\nconsists in dressing according to the views of one’s neighbour, whose\\nviews, as they are the views of the majority, will probably be extremely\\nstupid.  Or a man is called selfish if he lives in the manner that seems\\nto him most suitable for the full realisation of his own personality; if,\\nin fact, the primary aim of his life is self-development.  But this is\\nthe way in which everyone should live.  Selfishness is not living as one\\nwishes to live, it is asking others to live as one wishes to live.  And\\nunselfishness is letting other people’s lives alone, not interfering with\\nthem.  Selfishness always aims at creating around it an absolute\\nuniformity of type.  Unselfishness recognises infinite variety of type as\\na delightful thing, accepts it, acquiesces in it, enjoys it.  It is not\\nselfish to think for oneself.  A man who does not think for himself does\\nnot think at all.  It is grossly selfish to require of ones neighbour\\nthat he should think in the same way, and hold the same opinions.  Why\\nshould he?  If he can think, he will probably think differently.  If he\\ncannot think, it is monstrous to require thought of any kind from him.  A\\nred rose is not selfish because it wants to be a red rose.  It would be\\nhorribly selfish if it wanted all the other flowers in the garden to be\\nboth red and roses.  Under Individualism people will be quite natural and\\nabsolutely unselfish, and will know the meanings of the words, and\\nrealise them in their free, beautiful lives.  Nor will men be egotistic\\nas they are now.  For the egotist is he who makes claims upon others, and\\nthe Individualist will not desire to do that.  It will not give him\\npleasure.  When man has realised Individualism, he will also realise\\nsympathy and exercise it freely and spontaneously.  Up to the present man\\nhas hardly cultivated sympathy at all.  He has merely sympathy with pain,\\nand sympathy with pain is not the highest form of sympathy.  All sympathy\\nis fine, but sympathy with suffering is the least fine mode.  It is\\ntainted with egotism.  It is apt to become morbid.  There is in it a\\ncertain element of terror for our own safety.  We become afraid that we\\nourselves might be as the leper or as the blind, and that no man would\\nhave care of us.  It is curiously limiting, too.  One should sympathise\\nwith the entirety of life, not with life’s sores and maladies merely, but\\nwith life’s joy and beauty and energy and health and freedom.  The wider\\nsympathy is, of course, the more difficult.  It requires more\\nunselfishness.  Anybody can sympathise with the sufferings of a friend,\\nbut it requires a very fine nature—it requires, in fact, the nature of a\\ntrue Individualist—to sympathise with a friend’s success.\\n\\nIn the modern stress of competition and struggle for place, such sympathy\\nis naturally rare, and is also very much stifled by the immoral ideal of\\nuniformity of type and conformity to rule which is so prevalent\\neverywhere, and is perhaps most obnoxious in England.\\n\\nSympathy with pain there will, of course, always be.  It is one of the\\nfirst instincts of man.  The animals which are individual, the higher\\nanimals, that is to say, share it with us.  But it must be remembered\\nthat while sympathy with joy intensifies the sum of joy in the world,\\nsympathy with pain does not really diminish the amount of pain.  It may\\nmake man better able to endure evil, but the evil remains.  Sympathy with\\nconsumption does not cure consumption; that is what Science does.  And\\nwhen Socialism has solved the problem of poverty, and Science solved the\\nproblem of disease, the area of the sentimentalists will be lessened, and\\nthe sympathy of man will be large, healthy, and spontaneous.  Man will\\nhave joy in the contemplation of the joyous life of others.\\n\\nFor it is through joy that the Individualism of the future will develop\\nitself.  Christ made no attempt to reconstruct society, and consequently\\nthe Individualism that he preached to man could be realised only through\\npain or in solitude.  The ideals that we owe to Christ are the ideals of\\nthe man who abandons society entirely, or of the man who resists society\\nabsolutely.  But man is naturally social.  Even the Thebaid became\\npeopled at last.  And though the cenobite realises his personality, it is\\noften an impoverished personality that he so realises.  Upon the other\\nhand, the terrible truth that pain is a mode through which man may\\nrealise himself exercises a wonderful fascination over the world.\\nShallow speakers and shallow thinkers in pulpits and on platforms often\\ntalk about the world’s worship of pleasure, and whine against it.  But it\\nis rarely in the world’s history that its ideal has been one of joy and\\nbeauty.  The worship of pain has far more often dominated the world.\\nMediævalism, with its saints and martyrs, its love of self-torture, its\\nwild passion for wounding itself, its gashing with knives, and its\\nwhipping with rods—Mediævalism is real Christianity, and the mediæval\\nChrist is the real Christ.  When the Renaissance dawned upon the world,\\nand brought with it the new ideals of the beauty of life and the joy of\\nliving, men could not understand Christ.  Even Art shows us that.  The\\npainters of the Renaissance drew Christ as a little boy playing with\\nanother boy in a palace or a garden, or lying back in his mother’s arms,\\nsmiling at her, or at a flower, or at a bright bird; or as a noble,\\nstately figure moving nobly through the world; or as a wonderful figure\\nrising in a sort of ecstasy from death to life.  Even when they drew him\\ncrucified they drew him as a beautiful God on whom evil men had inflicted\\nsuffering.  But he did not preoccupy them much.  What delighted them was\\nto paint the men and women whom they admired, and to show the loveliness\\nof this lovely earth.  They painted many religious pictures—in fact, they\\npainted far too many, and the monotony of type and motive is wearisome,\\nand was bad for art.  It was the result of the authority of the public in\\nart-matters, and is to be deplored.  But their soul was not in the\\nsubject.  Raphael was a great artist when he painted his portrait of the\\nPope.  When he painted his Madonnas and infant Christs, he is not a great\\nartist at all.  Christ had no message for the Renaissance, which was\\nwonderful because it brought an ideal at variance with his, and to find\\nthe presentation of the real Christ we must go to mediæval art.  There he\\nis one maimed and marred; one who is not comely to look on, because\\nBeauty is a joy; one who is not in fair raiment, because that may be a\\njoy also: he is a beggar who has a marvellous soul; he is a leper whose\\nsoul is divine; he needs neither property nor health; he is a God\\nrealising his perfection through pain.\\n\\nThe evolution of man is slow.  The injustice of men is great.  It was\\nnecessary that pain should be put forward as a mode of self-realisation.\\nEven now, in some places in the world, the message of Christ is\\nnecessary.  No one who lived in modern Russia could possibly realise his\\nperfection except by pain.  A few Russian artists have realised\\nthemselves in Art; in a fiction that is mediæval in character, because\\nits dominant note is the realisation of men through suffering.  But for\\nthose who are not artists, and to whom there is no mode of life but the\\nactual life of fact, pain is the only door to perfection.  A Russian who\\nlives happily under the present system of government in Russia must\\neither believe that man has no soul, or that, if he has, it is not worth\\ndeveloping.  A Nihilist who rejects all authority, because he knows\\nauthority to be evil, and welcomes all pain, because through that he\\nrealises his personality, is a real Christian.  To him the Christian\\nideal is a true thing.\\n\\nAnd yet, Christ did not revolt against authority.  He accepted the\\nimperial authority of the Roman Empire and paid tribute.  He endured the\\necclesiastical authority of the Jewish Church, and would not repel its\\nviolence by any violence of his own.  He had, as I said before, no scheme\\nfor the reconstruction of society.  But the modern world has schemes.  It\\nproposes to do away with poverty and the suffering that it entails.  It\\ndesires to get rid of pain, and the suffering that pain entails.  It\\ntrusts to Socialism and to Science as its methods.  What it aims at is an\\nIndividualism expressing itself through joy.  This Individualism will be\\nlarger, fuller, lovelier than any Individualism has ever been.  Pain is\\nnot the ultimate mode of perfection.  It is merely provisional and a\\nprotest.  It has reference to wrong, unhealthy, unjust surroundings.\\nWhen the wrong, and the disease, and the injustice are removed, it will\\nhave no further place.  It will have done its work.  It was a great work,\\nbut it is almost over.  Its sphere lessens every day.\\n\\nNor will man miss it.  For what man has sought for is, indeed, neither\\npain nor pleasure, but simply Life.  Man has sought to live intensely,\\nfully, perfectly.  When he can do so without exercising restraint on\\nothers, or suffering it ever, and his activities are all pleasurable to\\nhim, he will be saner, healthier, more civilised, more himself.  Pleasure\\nis Nature’s test, her sign of approval.  When man is happy, he is in\\nharmony with himself and his environment.  The new Individualism, for\\nwhose service Socialism, whether it wills it or not, is working, will be\\nperfect harmony.  It will be what the Greeks sought for, but could not,\\nexcept in Thought, realise completely, because they had slaves, and fed\\nthem; it will be what the Renaissance sought for, but could not realise\\ncompletely except in Art, because they had slaves, and starved them.  It\\nwill be complete, and through it each man will attain to his perfection.\\nThe new Individualism is the new Hellenism.\\n\\n                                * * * * *\\n\\n               _Reprinted from the_ ‘_Fortnightly Review_,’\\n               _by permission of Messrs Chapman and Hall_.\\n\\n\\n\\n\\n***END OF THE PROJECT GUTENBERG EBOOK THE SOUL OF MAN***\\n\\n\\n******* This file should be named 1017-0.txt or 1017-0.zip *******\\n\\n\\nThis and all associated files of various formats will be found in:\\nhttp://www.gutenberg.org/dirs/1/0/1/1017\\n\\n\\nUpdated editions will replace the previous one--the old editions will\\nbe renamed.\\n\\nCreating the works from print editions not protected by U.S. copyright\\nlaw means that no one owns a United States copyright in these works,\\nso the Foundation (and you!) can copy and distribute it in the United\\nStates without permission and without paying copyright\\nroyalties. Special rules, set forth in the General Terms of Use part\\nof this license, apply to copying and distributing Project\\nGutenberg-tm electronic works to protect the PROJECT GUTENBERG-tm\\nconcept and trademark. Project Gutenberg is a registered trademark,\\nand may not be used if you charge for the eBooks, unless you receive\\nspecific permission. If you do not charge anything for copies of this\\neBook, complying with the rules is very easy. You may use this eBook\\nfor nearly any purpose such as creation of derivative works, reports,\\nperformances and research. They may be modified and printed and given\\naway--you may do practically ANYTHING in the United States with eBooks\\nnot protected by U.S. copyright law. Redistribution is subject to the\\ntrademark license, especially commercial redistribution.\\n\\nSTART: FULL LICENSE\\n\\nTHE FULL PROJECT GUTENBERG LICENSE\\nPLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\\n\\nTo protect the Project Gutenberg-tm mission of promoting the free\\ndistribution of electronic works, by using or distributing this work\\n(or any other work associated in any way with the phrase \"Project\\nGutenberg\"), you agree to comply with all the terms of the Full\\nProject Gutenberg-tm License available with this file or online at\\nwww.gutenberg.org/license.\\n\\nSection 1. General Terms of Use and Redistributing Project\\nGutenberg-tm electronic works\\n\\n1.A. By reading or using any part of this Project Gutenberg-tm\\nelectronic work, you indicate that you have read, understand, agree to\\nand accept all the terms of this license and intellectual property\\n(trademark/copyright) agreement. If you do not agree to abide by all\\nthe terms of this agreement, you must cease using and return or\\ndestroy all copies of Project Gutenberg-tm electronic works in your\\npossession. If you paid a fee for obtaining a copy of or access to a\\nProject Gutenberg-tm electronic work and you do not agree to be bound\\nby the terms of this agreement, you may obtain a refund from the\\nperson or entity to whom you paid the fee as set forth in paragraph\\n1.E.8.\\n\\n1.B. \"Project Gutenberg\" is a registered trademark. It may only be\\nused on or associated in any way with an electronic work by people who\\nagree to be bound by the terms of this agreement. There are a few\\nthings that you can do with most Project Gutenberg-tm electronic works\\neven without complying with the full terms of this agreement. See\\nparagraph 1.C below. There are a lot of things you can do with Project\\nGutenberg-tm electronic works if you follow the terms of this\\nagreement and help preserve free future access to Project Gutenberg-tm\\nelectronic works. See paragraph 1.E below.\\n\\n1.C. The Project Gutenberg Literary Archive Foundation (\"the\\nFoundation\" or PGLAF), owns a compilation copyright in the collection\\nof Project Gutenberg-tm electronic works. Nearly all the individual\\nworks in the collection are in the public domain in the United\\nStates. If an individual work is unprotected by copyright law in the\\nUnited States and you are located in the United States, we do not\\nclaim a right to prevent you from copying, distributing, performing,\\ndisplaying or creating derivative works based on the work as long as\\nall references to Project Gutenberg are removed. Of course, we hope\\nthat you will support the Project Gutenberg-tm mission of promoting\\nfree access to electronic works by freely sharing Project Gutenberg-tm\\nworks in compliance with the terms of this agreement for keeping the\\nProject Gutenberg-tm name associated with the work. You can easily\\ncomply with the terms of this agreement by keeping this work in the\\nsame format with its attached full Project Gutenberg-tm License when\\nyou share it without charge with others.\\n\\n1.D. The copyright laws of the place where you are located also govern\\nwhat you can do with this work. Copyright laws in most countries are\\nin a constant state of change. If you are outside the United States,\\ncheck the laws of your country in addition to the terms of this\\nagreement before downloading, copying, displaying, performing,\\ndistributing or creating derivative works based on this work or any\\nother Project Gutenberg-tm work. The Foundation makes no\\nrepresentations concerning the copyright status of any work in any\\ncountry outside the United States.\\n\\n1.E. Unless you have removed all references to Project Gutenberg:\\n\\n1.E.1. The following sentence, with active links to, or other\\nimmediate access to, the full Project Gutenberg-tm License must appear\\nprominently whenever any copy of a Project Gutenberg-tm work (any work\\non which the phrase \"Project Gutenberg\" appears, or with which the\\nphrase \"Project Gutenberg\" is associated) is accessed, displayed,\\nperformed, viewed, copied or distributed:\\n\\n  This eBook is for the use of anyone anywhere in the United States and\\n  most other parts of the world at no cost and with almost no\\n  restrictions whatsoever. You may copy it, give it away or re-use it\\n  under the terms of the Project Gutenberg License included with this\\n  eBook or online at www.gutenberg.org. If you are not located in the\\n  United States, you\\'ll have to check the laws of the country where you\\n  are located before using this ebook.\\n\\n1.E.2. If an individual Project Gutenberg-tm electronic work is\\nderived from texts not protected by U.S. copyright law (does not\\ncontain a notice indicating that it is posted with permission of the\\ncopyright holder), the work can be copied and distributed to anyone in\\nthe United States without paying any fees or charges. If you are\\nredistributing or providing access to a work with the phrase \"Project\\nGutenberg\" associated with or appearing on the work, you must comply\\neither with the requirements of paragraphs 1.E.1 through 1.E.7 or\\nobtain permission for the use of the work and the Project Gutenberg-tm\\ntrademark as set forth in paragraphs 1.E.8 or 1.E.9.\\n\\n1.E.3. If an individual Project Gutenberg-tm electronic work is posted\\nwith the permission of the copyright holder, your use and distribution\\nmust comply with both paragraphs 1.E.1 through 1.E.7 and any\\nadditional terms imposed by the copyright holder. Additional terms\\nwill be linked to the Project Gutenberg-tm License for all works\\nposted with the permission of the copyright holder found at the\\nbeginning of this work.\\n\\n1.E.4. Do not unlink or detach or remove the full Project Gutenberg-tm\\nLicense terms from this work, or any files containing a part of this\\nwork or any other work associated with Project Gutenberg-tm.\\n\\n1.E.5. Do not copy, display, perform, distribute or redistribute this\\nelectronic work, or any part of this electronic work, without\\nprominently displaying the sentence set forth in paragraph 1.E.1 with\\nactive links or immediate access to the full terms of the Project\\nGutenberg-tm License.\\n\\n1.E.6. You may convert to and distribute this work in any binary,\\ncompressed, marked up, nonproprietary or proprietary form, including\\nany word processing or hypertext form. However, if you provide access\\nto or distribute copies of a Project Gutenberg-tm work in a format\\nother than \"Plain Vanilla ASCII\" or other format used in the official\\nversion posted on the official Project Gutenberg-tm web site\\n(www.gutenberg.org), you must, at no additional cost, fee or expense\\nto the user, provide a copy, a means of exporting a copy, or a means\\nof obtaining a copy upon request, of the work in its original \"Plain\\nVanilla ASCII\" or other form. Any alternate format must include the\\nfull Project Gutenberg-tm License as specified in paragraph 1.E.1.\\n\\n1.E.7. Do not charge a fee for access to, viewing, displaying,\\nperforming, copying or distributing any Project Gutenberg-tm works\\nunless you comply with paragraph 1.E.8 or 1.E.9.\\n\\n1.E.8. You may charge a reasonable fee for copies of or providing\\naccess to or distributing Project Gutenberg-tm electronic works\\nprovided that\\n\\n* You pay a royalty fee of 20% of the gross profits you derive from\\n  the use of Project Gutenberg-tm works calculated using the method\\n  you already use to calculate your applicable taxes. The fee is owed\\n  to the owner of the Project Gutenberg-tm trademark, but he has\\n  agreed to donate royalties under this paragraph to the Project\\n  Gutenberg Literary Archive Foundation. Royalty payments must be paid\\n  within 60 days following each date on which you prepare (or are\\n  legally required to prepare) your periodic tax returns. Royalty\\n  payments should be clearly marked as such and sent to the Project\\n  Gutenberg Literary Archive Foundation at the address specified in\\n  Section 4, \"Information about donations to the Project Gutenberg\\n  Literary Archive Foundation.\"\\n\\n* You provide a full refund of any money paid by a user who notifies\\n  you in writing (or by e-mail) within 30 days of receipt that s/he\\n  does not agree to the terms of the full Project Gutenberg-tm\\n  License. You must require such a user to return or destroy all\\n  copies of the works possessed in a physical medium and discontinue\\n  all use of and all access to other copies of Project Gutenberg-tm\\n  works.\\n\\n* You provide, in accordance with paragraph 1.F.3, a full refund of\\n  any money paid for a work or a replacement copy, if a defect in the\\n  electronic work is discovered and reported to you within 90 days of\\n  receipt of the work.\\n\\n* You comply with all other terms of this agreement for free\\n  distribution of Project Gutenberg-tm works.\\n\\n1.E.9. If you wish to charge a fee or distribute a Project\\nGutenberg-tm electronic work or group of works on different terms than\\nare set forth in this agreement, you must obtain permission in writing\\nfrom both the Project Gutenberg Literary Archive Foundation and The\\nProject Gutenberg Trademark LLC, the owner of the Project Gutenberg-tm\\ntrademark. Contact the Foundation as set forth in Section 3 below.\\n\\n1.F.\\n\\n1.F.1. Project Gutenberg volunteers and employees expend considerable\\neffort to identify, do copyright research on, transcribe and proofread\\nworks not protected by U.S. copyright law in creating the Project\\nGutenberg-tm collection. Despite these efforts, Project Gutenberg-tm\\nelectronic works, and the medium on which they may be stored, may\\ncontain \"Defects,\" such as, but not limited to, incomplete, inaccurate\\nor corrupt data, transcription errors, a copyright or other\\nintellectual property infringement, a defective or damaged disk or\\nother medium, a computer virus, or computer codes that damage or\\ncannot be read by your equipment.\\n\\n1.F.2. LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the \"Right\\nof Replacement or Refund\" described in paragraph 1.F.3, the Project\\nGutenberg Literary Archive Foundation, the owner of the Project\\nGutenberg-tm trademark, and any other party distributing a Project\\nGutenberg-tm electronic work under this agreement, disclaim all\\nliability to you for damages, costs and expenses, including legal\\nfees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT\\nLIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\\nPROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE\\nTRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\\nLIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\\nINCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\\nDAMAGE.\\n\\n1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a\\ndefect in this electronic work within 90 days of receiving it, you can\\nreceive a refund of the money (if any) you paid for it by sending a\\nwritten explanation to the person you received the work from. If you\\nreceived the work on a physical medium, you must return the medium\\nwith your written explanation. The person or entity that provided you\\nwith the defective work may elect to provide a replacement copy in\\nlieu of a refund. If you received the work electronically, the person\\nor entity providing it to you may choose to give you a second\\nopportunity to receive the work electronically in lieu of a refund. If\\nthe second copy is also defective, you may demand a refund in writing\\nwithout further opportunities to fix the problem.\\n\\n1.F.4. Except for the limited right of replacement or refund set forth\\nin paragraph 1.F.3, this work is provided to you \\'AS-IS\\', WITH NO\\nOTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\\nLIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.\\n\\n1.F.5. Some states do not allow disclaimers of certain implied\\nwarranties or the exclusion or limitation of certain types of\\ndamages. If any disclaimer or limitation set forth in this agreement\\nviolates the law of the state applicable to this agreement, the\\nagreement shall be interpreted to make the maximum disclaimer or\\nlimitation permitted by the applicable state law. The invalidity or\\nunenforceability of any provision of this agreement shall not void the\\nremaining provisions.\\n\\n1.F.6. INDEMNITY - You agree to indemnify and hold the Foundation, the\\ntrademark owner, any agent or employee of the Foundation, anyone\\nproviding copies of Project Gutenberg-tm electronic works in\\naccordance with this agreement, and any volunteers associated with the\\nproduction, promotion and distribution of Project Gutenberg-tm\\nelectronic works, harmless from all liability, costs and expenses,\\nincluding legal fees, that arise directly or indirectly from any of\\nthe following which you do or cause to occur: (a) distribution of this\\nor any Project Gutenberg-tm work, (b) alteration, modification, or\\nadditions or deletions to any Project Gutenberg-tm work, and (c) any\\nDefect you cause.\\n\\nSection 2. Information about the Mission of Project Gutenberg-tm\\n\\nProject Gutenberg-tm is synonymous with the free distribution of\\nelectronic works in formats readable by the widest variety of\\ncomputers including obsolete, old, middle-aged and new computers. It\\nexists because of the efforts of hundreds of volunteers and donations\\nfrom people in all walks of life.\\n\\nVolunteers and financial support to provide volunteers with the\\nassistance they need are critical to reaching Project Gutenberg-tm\\'s\\ngoals and ensuring that the Project Gutenberg-tm collection will\\nremain freely available for generations to come. In 2001, the Project\\nGutenberg Literary Archive Foundation was created to provide a secure\\nand permanent future for Project Gutenberg-tm and future\\ngenerations. To learn more about the Project Gutenberg Literary\\nArchive Foundation and how your efforts and donations can help, see\\nSections 3 and 4 and the Foundation information page at\\nwww.gutenberg.org Section 3. Information about the Project Gutenberg\\nLiterary Archive Foundation\\n\\nThe Project Gutenberg Literary Archive Foundation is a non profit\\n501(c)(3) educational corporation organized under the laws of the\\nstate of Mississippi and granted tax exempt status by the Internal\\nRevenue Service. The Foundation\\'s EIN or federal tax identification\\nnumber is 64-6221541. Contributions to the Project Gutenberg Literary\\nArchive Foundation are tax deductible to the full extent permitted by\\nU.S. federal laws and your state\\'s laws.\\n\\nThe Foundation\\'s principal office is in Fairbanks, Alaska, with the\\nmailing address: PO Box 750175, Fairbanks, AK 99775, but its\\nvolunteers and employees are scattered throughout numerous\\nlocations. Its business office is located at 809 North 1500 West, Salt\\nLake City, UT 84116, (801) 596-1887. Email contact links and up to\\ndate contact information can be found at the Foundation\\'s web site and\\nofficial page at www.gutenberg.org/contact\\n\\nFor additional contact information:\\n\\n    Dr. Gregory B. Newby\\n    Chief Executive and Director\\n    gbnewby@pglaf.org\\n\\nSection 4. Information about Donations to the Project Gutenberg\\nLiterary Archive Foundation\\n\\nProject Gutenberg-tm depends upon and cannot survive without wide\\nspread public support and donations to carry out its mission of\\nincreasing the number of public domain and licensed works that can be\\nfreely distributed in machine readable form accessible by the widest\\narray of equipment including outdated equipment. Many small donations\\n($1 to $5,000) are particularly important to maintaining tax exempt\\nstatus with the IRS.\\n\\nThe Foundation is committed to complying with the laws regulating\\ncharities and charitable donations in all 50 states of the United\\nStates. Compliance requirements are not uniform and it takes a\\nconsiderable effort, much paperwork and many fees to meet and keep up\\nwith these requirements. We do not solicit donations in locations\\nwhere we have not received written confirmation of compliance. To SEND\\nDONATIONS or determine the status of compliance for any particular\\nstate visit www.gutenberg.org/donate\\n\\nWhile we cannot and do not solicit contributions from states where we\\nhave not met the solicitation requirements, we know of no prohibition\\nagainst accepting unsolicited donations from donors in such states who\\napproach us with offers to donate.\\n\\nInternational donations are gratefully accepted, but we cannot make\\nany statements concerning tax treatment of donations received from\\noutside the United States. U.S. laws alone swamp our small staff.\\n\\nPlease check the Project Gutenberg Web pages for current donation\\nmethods and addresses. Donations are accepted in a number of other\\nways including checks, online payments and credit card donations. To\\ndonate, please visit: www.gutenberg.org/donate\\n\\nSection 5. General Information About Project Gutenberg-tm electronic works.\\n\\nProfessor Michael S. Hart was the originator of the Project\\nGutenberg-tm concept of a library of electronic works that could be\\nfreely shared with anyone. For forty years, he produced and\\ndistributed Project Gutenberg-tm eBooks with only a loose network of\\nvolunteer support.\\n\\nProject Gutenberg-tm eBooks are often created from several printed\\neditions, all of which are confirmed as not protected by copyright in\\nthe U.S. unless a copyright notice is included. Thus, we do not\\nnecessarily keep eBooks in compliance with any particular paper\\nedition.\\n\\nMost people start at our Web site which has the main PG search\\nfacility: www.gutenberg.org\\n\\nThis Web site includes information about Project Gutenberg-tm,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\\n\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textLst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMX-Fu-GHdxj"
   },
   "source": [
    "## Process the text\n",
    "Initialize and fit the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zQf1AV8wHdxl"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(textLst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpZ0A2-xHdxp"
   },
   "source": [
    "### Vectorize the text\n",
    "\n",
    "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping words to numbers, and another for numbers to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_Nsq-rSPHdxq"
   },
   "outputs": [],
   "source": [
    "word_idx = tokenizer.word_index\n",
    "idx_word = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMYdjx4aHdxu"
   },
   "source": [
    "Get the word count for every word and also get the total number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ioEZ2c21Hdxw"
   },
   "outputs": [],
   "source": [
    "word_counts = tokenizer.word_counts\n",
    "num_words = len(word_idx) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWUBr9rHHdx0"
   },
   "source": [
    "Convert text to sequence of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dwLl0BWKHdx2"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(textLst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GkpK8McUHdx6"
   },
   "source": [
    "### Generate Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zxhQamjwHdx7"
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 50\n",
    "for seq in sequences:\n",
    "    for i in range(training_length, training_length+300):\n",
    "        extract = seq[i - training_length: i - training_length + 20]\n",
    "\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### The prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "Given a word, or a sequence of words, what is the most probable next word? This is the task we're training the model to perform. The input to the model will be a sequence of words, and we train the model to predict the output—the following word at each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the words computed until this moment, what is the next word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2bsVOl7HdyA"
   },
   "source": [
    "### Generate training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "j7-IsvynHdyB"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "features, labels = shuffle(features, labels, random_state=1)\n",
    "\n",
    "# Decide on number of samples for training\n",
    "train_end = int(0.7 * len(labels))\n",
    "\n",
    "train_features = np.array(features[:train_end])\n",
    "valid_features = np.array(features[train_end:])\n",
    "\n",
    "train_labels = labels[:train_end]\n",
    "valid_labels = labels[train_end:]\n",
    "\n",
    "# Convert to arrays\n",
    "X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
    "\n",
    "# Using int8 for memory savings\n",
    "y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
    "y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
    "\n",
    "# One hot encoding of labels\n",
    "for example_index, word_index in enumerate(train_labels):\n",
    "    y_train[example_index, word_index] = 1\n",
    "\n",
    "for example_index, word_index in enumerate(valid_labels):\n",
    "    y_valid[example_index, word_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juT1mZrUHdyE"
   },
   "source": [
    "This is just to check the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "wkdmNbgjHdyF",
    "outputId": "a5880d2a-979a-4c13-d2b9-469710e7dad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: co london first printed in this edition april 1913 reprinted june 1913 september 1913 june 1914 january 1916 october\n",
      "\n",
      "Label: 1916\n",
      "\n",
      "Features: whom beautiful things mean only beauty there is no such thing as a moral or an immoral book books\n",
      "\n",
      "Label: are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sequence in enumerate(X_train[:2]):\n",
    "    text = []\n",
    "#     print(i, sequence)\n",
    "    for idx in sequence:\n",
    "        text.append(idx_word[idx])\n",
    "        \n",
    "    print('Features: ' + ' '.join(text)+'\\n')\n",
    "    print('Label: ' + idx_word[np.argmax(y_train[i])] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "Use `keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "* `keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
    "* `keras.layers.LSTM`: A type of RNN with size `units=rnn_units` (You can also use a GRU layer here.)\n",
    "* `keras.layers.Dense`: The output layer, with `num_words` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "GKpCQFZLHdyN",
    "outputId": "69e4effb-fb07-4db8-f6a5-d34c97b4ecee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1478800   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14788)             961220    \n",
      "=================================================================\n",
      "Total params: 2,486,420\n",
      "Trainable params: 2,486,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=len(word_idx) + 1,\n",
    "        output_dim=100,\n",
    "        weights=None,\n",
    "        trainable=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64, return_sequences=False, dropout=0.1,\n",
    "        recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vL3tUp1UHdyS"
   },
   "source": [
    "For each word the model looks up the embedding, runs the LSTM one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-liklihood of the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 18500
    },
    "colab_type": "code",
    "id": "6o84puBcHdyV",
    "outputId": "56328367-3a61-42ce-fd98-787df86bdaf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 9.2461 - acc: 0.0376\n",
      "Epoch 2/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6.8476 - acc: 0.0562\n",
      "Epoch 3/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6.4580 - acc: 0.0610\n",
      "Epoch 4/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6.2851 - acc: 0.0638\n",
      "Epoch 5/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6.1510 - acc: 0.0648\n",
      "Epoch 6/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6.0644 - acc: 0.0633\n",
      "Epoch 7/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.9788 - acc: 0.0671\n",
      "Epoch 8/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.9023 - acc: 0.0614\n",
      "Epoch 9/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.8504 - acc: 0.0633\n",
      "Epoch 10/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.8090 - acc: 0.0638\n",
      "Epoch 11/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.7623 - acc: 0.0600\n",
      "Epoch 12/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.7371 - acc: 0.0600\n",
      "Epoch 13/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.7309 - acc: 0.0614\n",
      "Epoch 14/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.6893 - acc: 0.0614\n",
      "Epoch 15/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.6627 - acc: 0.0643\n",
      "Epoch 16/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.6387 - acc: 0.0652\n",
      "Epoch 17/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.6202 - acc: 0.0710\n",
      "Epoch 18/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.5673 - acc: 0.0738\n",
      "Epoch 19/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.5642 - acc: 0.0729\n",
      "Epoch 20/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.5201 - acc: 0.0767\n",
      "Epoch 21/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.4676 - acc: 0.0705\n",
      "Epoch 22/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.4379 - acc: 0.0757\n",
      "Epoch 23/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.3854 - acc: 0.0748\n",
      "Epoch 24/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.3475 - acc: 0.0781\n",
      "Epoch 25/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.3016 - acc: 0.0790\n",
      "Epoch 26/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.2642 - acc: 0.0833\n",
      "Epoch 27/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.2132 - acc: 0.0800\n",
      "Epoch 28/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.1739 - acc: 0.0814\n",
      "Epoch 29/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.1026 - acc: 0.0905\n",
      "Epoch 30/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.0464 - acc: 0.0929\n",
      "Epoch 31/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5.0023 - acc: 0.0919\n",
      "Epoch 32/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.9331 - acc: 0.1005\n",
      "Epoch 33/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.8805 - acc: 0.1057\n",
      "Epoch 34/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.8216 - acc: 0.1100\n",
      "Epoch 35/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.7859 - acc: 0.1129\n",
      "Epoch 36/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.7168 - acc: 0.1152\n",
      "Epoch 37/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.6419 - acc: 0.1190\n",
      "Epoch 38/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.5898 - acc: 0.1195\n",
      "Epoch 39/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.5061 - acc: 0.1367\n",
      "Epoch 40/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.4493 - acc: 0.1357\n",
      "Epoch 41/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.3430 - acc: 0.1395\n",
      "Epoch 42/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.2861 - acc: 0.1548\n",
      "Epoch 43/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.1834 - acc: 0.1595\n",
      "Epoch 44/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.1123 - acc: 0.1686\n",
      "Epoch 45/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 4.0104 - acc: 0.1824\n",
      "Epoch 46/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.9490 - acc: 0.1876\n",
      "Epoch 47/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.8810 - acc: 0.2014\n",
      "Epoch 48/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.8224 - acc: 0.1957\n",
      "Epoch 49/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.7261 - acc: 0.2105\n",
      "Epoch 50/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.6425 - acc: 0.2190\n",
      "Epoch 51/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.6036 - acc: 0.2295\n",
      "Epoch 52/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.5387 - acc: 0.2357\n",
      "Epoch 53/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.4918 - acc: 0.2452\n",
      "Epoch 54/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.4087 - acc: 0.2643\n",
      "Epoch 55/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.3319 - acc: 0.2633\n",
      "Epoch 56/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.2744 - acc: 0.2719\n",
      "Epoch 57/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.2129 - acc: 0.2819\n",
      "Epoch 58/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.1955 - acc: 0.2862\n",
      "Epoch 59/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.1536 - acc: 0.2819\n",
      "Epoch 60/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 3.0548 - acc: 0.3067\n",
      "Epoch 61/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.9711 - acc: 0.3076\n",
      "Epoch 62/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.9532 - acc: 0.3119\n",
      "Epoch 63/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.9120 - acc: 0.3162\n",
      "Epoch 64/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.8518 - acc: 0.3433\n",
      "Epoch 65/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.8138 - acc: 0.3414\n",
      "Epoch 66/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.7453 - acc: 0.3467\n",
      "Epoch 67/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.6748 - acc: 0.3600\n",
      "Epoch 68/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.6442 - acc: 0.3600\n",
      "Epoch 69/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.5669 - acc: 0.3810\n",
      "Epoch 70/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.5327 - acc: 0.3790\n",
      "Epoch 71/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.5292 - acc: 0.3914\n",
      "Epoch 72/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.4610 - acc: 0.3948\n",
      "Epoch 73/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.4554 - acc: 0.3919\n",
      "Epoch 74/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.4269 - acc: 0.3886\n",
      "Epoch 75/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.3491 - acc: 0.4114\n",
      "Epoch 76/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.3184 - acc: 0.4257\n",
      "Epoch 77/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.2763 - acc: 0.4205\n",
      "Epoch 78/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.2546 - acc: 0.4190\n",
      "Epoch 79/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.1898 - acc: 0.4252\n",
      "Epoch 80/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.1690 - acc: 0.4481\n",
      "Epoch 81/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.1158 - acc: 0.4590\n",
      "Epoch 82/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.1115 - acc: 0.4567\n",
      "Epoch 83/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.0445 - acc: 0.4700\n",
      "Epoch 84/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.0026 - acc: 0.4790\n",
      "Epoch 85/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 2.0239 - acc: 0.4662\n",
      "Epoch 86/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.9794 - acc: 0.4781\n",
      "Epoch 87/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.8811 - acc: 0.4910\n",
      "Epoch 88/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.8755 - acc: 0.5086\n",
      "Epoch 89/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.8538 - acc: 0.5090\n",
      "Epoch 90/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.8495 - acc: 0.5105\n",
      "Epoch 91/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.7889 - acc: 0.5219\n",
      "Epoch 92/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.7908 - acc: 0.5310\n",
      "Epoch 93/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.7367 - acc: 0.5333\n",
      "Epoch 94/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.7371 - acc: 0.5381\n",
      "Epoch 95/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.6888 - acc: 0.5433\n",
      "Epoch 96/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.6823 - acc: 0.5424\n",
      "Epoch 97/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.6754 - acc: 0.5405\n",
      "Epoch 98/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.6387 - acc: 0.5605\n",
      "Epoch 99/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.6553 - acc: 0.5548\n",
      "Epoch 100/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.5725 - acc: 0.5657\n",
      "Epoch 101/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.5570 - acc: 0.5771\n",
      "Epoch 102/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.5168 - acc: 0.5805\n",
      "Epoch 103/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.5121 - acc: 0.5776\n",
      "Epoch 104/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.4671 - acc: 0.5886\n",
      "Epoch 105/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.4605 - acc: 0.5876\n",
      "Epoch 106/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.4807 - acc: 0.5852\n",
      "Epoch 107/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.4281 - acc: 0.5971\n",
      "Epoch 108/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.4197 - acc: 0.6148\n",
      "Epoch 109/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.4125 - acc: 0.5986\n",
      "Epoch 110/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.3556 - acc: 0.6048\n",
      "Epoch 111/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.3696 - acc: 0.6005\n",
      "Epoch 112/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.3736 - acc: 0.6095\n",
      "Epoch 113/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.3600 - acc: 0.6076\n",
      "Epoch 114/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.3285 - acc: 0.6176\n",
      "Epoch 115/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.3021 - acc: 0.6414\n",
      "Epoch 116/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.3029 - acc: 0.6290\n",
      "Epoch 117/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.2906 - acc: 0.6386\n",
      "Epoch 118/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.2738 - acc: 0.6300\n",
      "Epoch 119/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.2166 - acc: 0.6471\n",
      "Epoch 120/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.2323 - acc: 0.6495\n",
      "Epoch 121/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.2103 - acc: 0.6633\n",
      "Epoch 122/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.1757 - acc: 0.6600\n",
      "Epoch 123/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.1606 - acc: 0.6576\n",
      "Epoch 124/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.1494 - acc: 0.6719\n",
      "Epoch 125/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.1752 - acc: 0.6690\n",
      "Epoch 126/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.1398 - acc: 0.6710\n",
      "Epoch 127/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.1136 - acc: 0.6871\n",
      "Epoch 128/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.1135 - acc: 0.6843\n",
      "Epoch 129/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0880 - acc: 0.6662\n",
      "Epoch 130/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.1076 - acc: 0.6838\n",
      "Epoch 131/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0796 - acc: 0.6962\n",
      "Epoch 132/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0631 - acc: 0.6871\n",
      "Epoch 133/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0721 - acc: 0.6929\n",
      "Epoch 134/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0926 - acc: 0.6743\n",
      "Epoch 135/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0041 - acc: 0.7186\n",
      "Epoch 136/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0067 - acc: 0.7048\n",
      "Epoch 137/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0093 - acc: 0.7100\n",
      "Epoch 138/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0150 - acc: 0.7019\n",
      "Epoch 139/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.9757 - acc: 0.7152\n",
      "Epoch 140/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.9900 - acc: 0.7124\n",
      "Epoch 141/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 1.0018 - acc: 0.6981\n",
      "Epoch 142/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.9893 - acc: 0.7081\n",
      "Epoch 143/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.9597 - acc: 0.7243\n",
      "Epoch 144/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.9666 - acc: 0.7062\n",
      "Epoch 145/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.9377 - acc: 0.7195\n",
      "Epoch 146/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.9341 - acc: 0.7310\n",
      "Epoch 147/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.9041 - acc: 0.7310\n",
      "Epoch 148/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.9024 - acc: 0.7352\n",
      "Epoch 149/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8976 - acc: 0.7300\n",
      "Epoch 150/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8724 - acc: 0.7467\n",
      "Epoch 151/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8983 - acc: 0.7333\n",
      "Epoch 152/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8846 - acc: 0.7386\n",
      "Epoch 153/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8561 - acc: 0.7495\n",
      "Epoch 154/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8526 - acc: 0.7448\n",
      "Epoch 155/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8477 - acc: 0.7481\n",
      "Epoch 156/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8334 - acc: 0.7500\n",
      "Epoch 157/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8110 - acc: 0.7543\n",
      "Epoch 158/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8447 - acc: 0.7395\n",
      "Epoch 159/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8408 - acc: 0.7524\n",
      "Epoch 160/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7995 - acc: 0.7571\n",
      "Epoch 161/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7905 - acc: 0.7638\n",
      "Epoch 162/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8109 - acc: 0.7538\n",
      "Epoch 163/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7979 - acc: 0.7643\n",
      "Epoch 164/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.8024 - acc: 0.7648\n",
      "Epoch 165/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7908 - acc: 0.7657\n",
      "Epoch 166/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7502 - acc: 0.7724\n",
      "Epoch 167/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7414 - acc: 0.7833\n",
      "Epoch 168/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7561 - acc: 0.7714\n",
      "Epoch 169/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7396 - acc: 0.7824\n",
      "Epoch 170/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7842 - acc: 0.7562\n",
      "Epoch 171/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7498 - acc: 0.7710\n",
      "Epoch 172/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7491 - acc: 0.7757\n",
      "Epoch 173/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7574 - acc: 0.7695\n",
      "Epoch 174/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7646 - acc: 0.7652\n",
      "Epoch 175/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7470 - acc: 0.7752\n",
      "Epoch 176/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7493 - acc: 0.7719\n",
      "Epoch 177/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7097 - acc: 0.7781\n",
      "Epoch 178/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7077 - acc: 0.7895\n",
      "Epoch 179/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7147 - acc: 0.7843\n",
      "Epoch 180/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6881 - acc: 0.7948\n",
      "Epoch 181/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7045 - acc: 0.7819\n",
      "Epoch 182/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7007 - acc: 0.7843\n",
      "Epoch 183/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6808 - acc: 0.8014\n",
      "Epoch 184/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7416 - acc: 0.7671\n",
      "Epoch 185/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.7188 - acc: 0.7781\n",
      "Epoch 186/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6896 - acc: 0.7914\n",
      "Epoch 187/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6805 - acc: 0.7929\n",
      "Epoch 188/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6869 - acc: 0.7881\n",
      "Epoch 189/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6891 - acc: 0.7881\n",
      "Epoch 190/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6356 - acc: 0.7976\n",
      "Epoch 191/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6570 - acc: 0.8052\n",
      "Epoch 192/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6561 - acc: 0.7876\n",
      "Epoch 193/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6072 - acc: 0.8095\n",
      "Epoch 194/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6336 - acc: 0.8105\n",
      "Epoch 195/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6305 - acc: 0.8024\n",
      "Epoch 196/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6459 - acc: 0.7976\n",
      "Epoch 197/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6534 - acc: 0.7948\n",
      "Epoch 198/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6219 - acc: 0.8129\n",
      "Epoch 199/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6572 - acc: 0.7857\n",
      "Epoch 200/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6043 - acc: 0.8224\n",
      "Epoch 201/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6231 - acc: 0.8038\n",
      "Epoch 202/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6072 - acc: 0.8090\n",
      "Epoch 203/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6354 - acc: 0.8038\n",
      "Epoch 204/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5982 - acc: 0.8071\n",
      "Epoch 205/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6075 - acc: 0.8138\n",
      "Epoch 206/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6207 - acc: 0.8124\n",
      "Epoch 207/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.6711 - acc: 0.8019\n",
      "Epoch 208/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5810 - acc: 0.8262\n",
      "Epoch 209/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5934 - acc: 0.8129\n",
      "Epoch 210/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5791 - acc: 0.8233\n",
      "Epoch 211/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5744 - acc: 0.8229\n",
      "Epoch 212/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5612 - acc: 0.8219\n",
      "Epoch 213/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5645 - acc: 0.8252\n",
      "Epoch 214/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5822 - acc: 0.8181\n",
      "Epoch 215/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5947 - acc: 0.8071\n",
      "Epoch 216/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5618 - acc: 0.8243\n",
      "Epoch 217/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5818 - acc: 0.8176\n",
      "Epoch 218/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5823 - acc: 0.8148\n",
      "Epoch 219/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5180 - acc: 0.8405\n",
      "Epoch 220/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5246 - acc: 0.8324\n",
      "Epoch 221/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5808 - acc: 0.8171\n",
      "Epoch 222/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5676 - acc: 0.8152\n",
      "Epoch 223/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5386 - acc: 0.8343\n",
      "Epoch 224/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5313 - acc: 0.8333\n",
      "Epoch 225/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5394 - acc: 0.8295\n",
      "Epoch 226/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5336 - acc: 0.8357\n",
      "Epoch 227/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5203 - acc: 0.8319\n",
      "Epoch 228/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5535 - acc: 0.8252\n",
      "Epoch 229/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5220 - acc: 0.8376\n",
      "Epoch 230/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5247 - acc: 0.8371\n",
      "Epoch 231/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5509 - acc: 0.8271\n",
      "Epoch 232/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5218 - acc: 0.8333\n",
      "Epoch 233/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5121 - acc: 0.8348\n",
      "Epoch 234/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5081 - acc: 0.8471\n",
      "Epoch 235/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5036 - acc: 0.8386\n",
      "Epoch 236/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5211 - acc: 0.8286\n",
      "Epoch 237/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4936 - acc: 0.8486\n",
      "Epoch 238/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5146 - acc: 0.8362\n",
      "Epoch 239/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5166 - acc: 0.8367\n",
      "Epoch 240/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5371 - acc: 0.8229\n",
      "Epoch 241/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4790 - acc: 0.8481\n",
      "Epoch 242/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5240 - acc: 0.8290\n",
      "Epoch 243/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4980 - acc: 0.8467\n",
      "Epoch 244/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5373 - acc: 0.8314\n",
      "Epoch 245/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4766 - acc: 0.8433\n",
      "Epoch 246/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5027 - acc: 0.8352\n",
      "Epoch 247/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.5195 - acc: 0.8390\n",
      "Epoch 248/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4885 - acc: 0.8457\n",
      "Epoch 249/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4646 - acc: 0.8552\n",
      "Epoch 250/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4746 - acc: 0.8519\n",
      "Epoch 251/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4311 - acc: 0.8605\n",
      "Epoch 252/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4434 - acc: 0.8676\n",
      "Epoch 253/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4702 - acc: 0.8471\n",
      "Epoch 254/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4504 - acc: 0.8562\n",
      "Epoch 255/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4701 - acc: 0.8495\n",
      "Epoch 256/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4557 - acc: 0.8562\n",
      "Epoch 257/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4634 - acc: 0.8600\n",
      "Epoch 258/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4550 - acc: 0.8586\n",
      "Epoch 259/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4627 - acc: 0.8524\n",
      "Epoch 260/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4266 - acc: 0.8643\n",
      "Epoch 261/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4467 - acc: 0.8562\n",
      "Epoch 262/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4673 - acc: 0.8524\n",
      "Epoch 263/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4768 - acc: 0.8457\n",
      "Epoch 264/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4789 - acc: 0.8476\n",
      "Epoch 265/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4664 - acc: 0.8505\n",
      "Epoch 266/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4559 - acc: 0.8548\n",
      "Epoch 267/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4886 - acc: 0.8481\n",
      "Epoch 268/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4254 - acc: 0.8643\n",
      "Epoch 269/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4248 - acc: 0.8686\n",
      "Epoch 270/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4624 - acc: 0.8448\n",
      "Epoch 271/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4295 - acc: 0.8648\n",
      "Epoch 272/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4440 - acc: 0.8624\n",
      "Epoch 273/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4369 - acc: 0.8671\n",
      "Epoch 274/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4249 - acc: 0.8657\n",
      "Epoch 275/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4318 - acc: 0.8690\n",
      "Epoch 276/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4531 - acc: 0.8643\n",
      "Epoch 277/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4497 - acc: 0.8543\n",
      "Epoch 278/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4416 - acc: 0.8595\n",
      "Epoch 279/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4114 - acc: 0.8681\n",
      "Epoch 280/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4749 - acc: 0.8429\n",
      "Epoch 281/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4363 - acc: 0.8643\n",
      "Epoch 282/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4499 - acc: 0.8600\n",
      "Epoch 283/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4115 - acc: 0.8676\n",
      "Epoch 284/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4247 - acc: 0.8643\n",
      "Epoch 285/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4162 - acc: 0.8633\n",
      "Epoch 286/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4121 - acc: 0.8681\n",
      "Epoch 287/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4228 - acc: 0.8614\n",
      "Epoch 288/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4198 - acc: 0.8671\n",
      "Epoch 289/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4157 - acc: 0.8686\n",
      "Epoch 290/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4464 - acc: 0.8600\n",
      "Epoch 291/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3918 - acc: 0.8795\n",
      "Epoch 292/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4206 - acc: 0.8686\n",
      "Epoch 293/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4231 - acc: 0.8619\n",
      "Epoch 294/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4220 - acc: 0.8629\n",
      "Epoch 295/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3763 - acc: 0.8771\n",
      "Epoch 296/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3841 - acc: 0.8657\n",
      "Epoch 297/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3669 - acc: 0.8833\n",
      "Epoch 298/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3975 - acc: 0.8776\n",
      "Epoch 299/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3824 - acc: 0.8771\n",
      "Epoch 300/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4178 - acc: 0.8667\n",
      "Epoch 301/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3983 - acc: 0.8705\n",
      "Epoch 302/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3738 - acc: 0.8843\n",
      "Epoch 303/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3973 - acc: 0.8748\n",
      "Epoch 304/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4545 - acc: 0.8538\n",
      "Epoch 305/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3947 - acc: 0.8762\n",
      "Epoch 306/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3859 - acc: 0.8729\n",
      "Epoch 307/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4052 - acc: 0.8743\n",
      "Epoch 308/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3787 - acc: 0.8733\n",
      "Epoch 309/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3708 - acc: 0.8795\n",
      "Epoch 310/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3881 - acc: 0.8671\n",
      "Epoch 311/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3929 - acc: 0.8752\n",
      "Epoch 312/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3627 - acc: 0.8814\n",
      "Epoch 313/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3754 - acc: 0.8738\n",
      "Epoch 314/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3841 - acc: 0.8762\n",
      "Epoch 315/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3682 - acc: 0.8848\n",
      "Epoch 316/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3584 - acc: 0.8871\n",
      "Epoch 317/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3537 - acc: 0.8810\n",
      "Epoch 318/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3751 - acc: 0.8824\n",
      "Epoch 319/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3744 - acc: 0.8795\n",
      "Epoch 320/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3646 - acc: 0.8829\n",
      "Epoch 321/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3820 - acc: 0.8743\n",
      "Epoch 322/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3727 - acc: 0.8748\n",
      "Epoch 323/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3634 - acc: 0.8771\n",
      "Epoch 324/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3442 - acc: 0.8857\n",
      "Epoch 325/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3679 - acc: 0.8810\n",
      "Epoch 326/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3682 - acc: 0.8776\n",
      "Epoch 327/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3502 - acc: 0.8824\n",
      "Epoch 328/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3625 - acc: 0.8838\n",
      "Epoch 329/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3634 - acc: 0.8767\n",
      "Epoch 330/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3603 - acc: 0.8843\n",
      "Epoch 331/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3774 - acc: 0.8810\n",
      "Epoch 332/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3394 - acc: 0.8957\n",
      "Epoch 333/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3779 - acc: 0.8776\n",
      "Epoch 334/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.4032 - acc: 0.8695\n",
      "Epoch 335/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3504 - acc: 0.8867\n",
      "Epoch 336/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3531 - acc: 0.8857\n",
      "Epoch 337/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3375 - acc: 0.8852\n",
      "Epoch 338/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3569 - acc: 0.8895\n",
      "Epoch 339/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3801 - acc: 0.8838\n",
      "Epoch 340/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3486 - acc: 0.8838\n",
      "Epoch 341/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3548 - acc: 0.8886\n",
      "Epoch 342/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3622 - acc: 0.8848\n",
      "Epoch 343/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3561 - acc: 0.8805\n",
      "Epoch 344/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3404 - acc: 0.8876\n",
      "Epoch 345/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3239 - acc: 0.8995\n",
      "Epoch 346/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3356 - acc: 0.8914\n",
      "Epoch 347/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3600 - acc: 0.8762\n",
      "Epoch 348/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3433 - acc: 0.8833\n",
      "Epoch 349/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3430 - acc: 0.8886\n",
      "Epoch 350/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3650 - acc: 0.8743\n",
      "Epoch 351/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3435 - acc: 0.8871\n",
      "Epoch 352/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3363 - acc: 0.8910\n",
      "Epoch 353/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3326 - acc: 0.8895\n",
      "Epoch 354/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3637 - acc: 0.8838\n",
      "Epoch 355/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2908 - acc: 0.9024\n",
      "Epoch 356/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3646 - acc: 0.8790\n",
      "Epoch 357/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3512 - acc: 0.8814\n",
      "Epoch 358/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3288 - acc: 0.8910\n",
      "Epoch 359/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3165 - acc: 0.9000\n",
      "Epoch 360/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3090 - acc: 0.8924\n",
      "Epoch 361/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3338 - acc: 0.8962\n",
      "Epoch 362/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3487 - acc: 0.8810\n",
      "Epoch 363/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3216 - acc: 0.8895\n",
      "Epoch 364/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2931 - acc: 0.9071\n",
      "Epoch 365/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3351 - acc: 0.8819\n",
      "Epoch 366/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3179 - acc: 0.8910\n",
      "Epoch 367/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3354 - acc: 0.8962\n",
      "Epoch 368/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3382 - acc: 0.8886\n",
      "Epoch 369/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3377 - acc: 0.8881\n",
      "Epoch 370/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3131 - acc: 0.8914\n",
      "Epoch 371/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3135 - acc: 0.8962\n",
      "Epoch 372/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3237 - acc: 0.8919\n",
      "Epoch 373/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3230 - acc: 0.8881\n",
      "Epoch 374/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3324 - acc: 0.8924\n",
      "Epoch 375/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3135 - acc: 0.9005\n",
      "Epoch 376/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3051 - acc: 0.8976\n",
      "Epoch 377/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3352 - acc: 0.8871\n",
      "Epoch 378/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3289 - acc: 0.8895\n",
      "Epoch 379/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3008 - acc: 0.8971\n",
      "Epoch 380/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2899 - acc: 0.9048\n",
      "Epoch 381/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2972 - acc: 0.9029\n",
      "Epoch 382/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3245 - acc: 0.8924\n",
      "Epoch 383/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2906 - acc: 0.9024\n",
      "Epoch 384/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3354 - acc: 0.8895\n",
      "Epoch 385/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3023 - acc: 0.9043\n",
      "Epoch 386/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3174 - acc: 0.8938\n",
      "Epoch 387/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3092 - acc: 0.8910\n",
      "Epoch 388/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3150 - acc: 0.9010\n",
      "Epoch 389/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2948 - acc: 0.9010\n",
      "Epoch 390/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3009 - acc: 0.8995\n",
      "Epoch 391/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2723 - acc: 0.9100\n",
      "Epoch 392/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3049 - acc: 0.8943\n",
      "Epoch 393/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3058 - acc: 0.9000\n",
      "Epoch 394/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2841 - acc: 0.9062\n",
      "Epoch 395/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2701 - acc: 0.9114\n",
      "Epoch 396/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3010 - acc: 0.9043\n",
      "Epoch 397/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3212 - acc: 0.8886\n",
      "Epoch 398/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3070 - acc: 0.9000\n",
      "Epoch 399/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3047 - acc: 0.9000\n",
      "Epoch 400/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3130 - acc: 0.9000\n",
      "Epoch 401/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2806 - acc: 0.9067\n",
      "Epoch 402/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3199 - acc: 0.8943\n",
      "Epoch 403/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3134 - acc: 0.8924\n",
      "Epoch 404/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2941 - acc: 0.9019\n",
      "Epoch 405/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2820 - acc: 0.9052\n",
      "Epoch 406/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3325 - acc: 0.8910\n",
      "Epoch 407/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3163 - acc: 0.9052\n",
      "Epoch 408/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3101 - acc: 0.9014\n",
      "Epoch 409/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3078 - acc: 0.9005\n",
      "Epoch 410/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3008 - acc: 0.9000\n",
      "Epoch 411/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2798 - acc: 0.9029\n",
      "Epoch 412/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2861 - acc: 0.9048\n",
      "Epoch 413/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2745 - acc: 0.9100\n",
      "Epoch 414/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2875 - acc: 0.9062\n",
      "Epoch 415/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2908 - acc: 0.9081\n",
      "Epoch 416/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2760 - acc: 0.9014\n",
      "Epoch 417/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2913 - acc: 0.9038\n",
      "Epoch 418/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2863 - acc: 0.9081\n",
      "Epoch 419/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2765 - acc: 0.9105\n",
      "Epoch 420/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2910 - acc: 0.9019\n",
      "Epoch 421/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2947 - acc: 0.9062\n",
      "Epoch 422/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2968 - acc: 0.9029\n",
      "Epoch 423/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2963 - acc: 0.9071\n",
      "Epoch 424/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2831 - acc: 0.9119\n",
      "Epoch 425/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3008 - acc: 0.9000\n",
      "Epoch 426/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2647 - acc: 0.9062\n",
      "Epoch 427/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2816 - acc: 0.9019\n",
      "Epoch 428/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3043 - acc: 0.9024\n",
      "Epoch 429/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2946 - acc: 0.9010\n",
      "Epoch 430/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.3003 - acc: 0.9076\n",
      "Epoch 431/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2905 - acc: 0.9048\n",
      "Epoch 432/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2739 - acc: 0.9114\n",
      "Epoch 433/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2501 - acc: 0.9181\n",
      "Epoch 434/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2785 - acc: 0.9000\n",
      "Epoch 435/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2749 - acc: 0.9124\n",
      "Epoch 436/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2603 - acc: 0.9157\n",
      "Epoch 437/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2692 - acc: 0.9076\n",
      "Epoch 438/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2681 - acc: 0.9143\n",
      "Epoch 439/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2899 - acc: 0.9062\n",
      "Epoch 440/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2807 - acc: 0.9067\n",
      "Epoch 441/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2928 - acc: 0.8929\n",
      "Epoch 442/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2989 - acc: 0.9019\n",
      "Epoch 443/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2645 - acc: 0.9133\n",
      "Epoch 444/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2820 - acc: 0.9081\n",
      "Epoch 445/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2923 - acc: 0.9048\n",
      "Epoch 446/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2655 - acc: 0.9081\n",
      "Epoch 447/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2771 - acc: 0.9086\n",
      "Epoch 448/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2647 - acc: 0.9114\n",
      "Epoch 449/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2673 - acc: 0.9057\n",
      "Epoch 450/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2512 - acc: 0.9138\n",
      "Epoch 451/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2839 - acc: 0.9052\n",
      "Epoch 452/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2535 - acc: 0.9138\n",
      "Epoch 453/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2910 - acc: 0.9014\n",
      "Epoch 454/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2694 - acc: 0.9162\n",
      "Epoch 455/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2883 - acc: 0.9081\n",
      "Epoch 456/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2781 - acc: 0.9081\n",
      "Epoch 457/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2652 - acc: 0.9095\n",
      "Epoch 458/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2668 - acc: 0.9086\n",
      "Epoch 459/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2541 - acc: 0.9148\n",
      "Epoch 460/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2836 - acc: 0.9033\n",
      "Epoch 461/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2689 - acc: 0.9119\n",
      "Epoch 462/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2762 - acc: 0.9100\n",
      "Epoch 463/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2996 - acc: 0.9052\n",
      "Epoch 464/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2620 - acc: 0.9167\n",
      "Epoch 465/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2753 - acc: 0.9114\n",
      "Epoch 466/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2621 - acc: 0.9110\n",
      "Epoch 467/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2507 - acc: 0.9214\n",
      "Epoch 468/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2563 - acc: 0.9171\n",
      "Epoch 469/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2614 - acc: 0.9076\n",
      "Epoch 470/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2689 - acc: 0.9043\n",
      "Epoch 471/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2283 - acc: 0.9243\n",
      "Epoch 472/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2717 - acc: 0.9081\n",
      "Epoch 473/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2470 - acc: 0.9190\n",
      "Epoch 474/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2461 - acc: 0.9167\n",
      "Epoch 475/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2548 - acc: 0.9143\n",
      "Epoch 476/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2649 - acc: 0.9071\n",
      "Epoch 477/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2768 - acc: 0.9105\n",
      "Epoch 478/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2777 - acc: 0.9071\n",
      "Epoch 479/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2513 - acc: 0.9157\n",
      "Epoch 480/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2552 - acc: 0.9167\n",
      "Epoch 481/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2960 - acc: 0.8971\n",
      "Epoch 482/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2587 - acc: 0.9100\n",
      "Epoch 483/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2705 - acc: 0.9110\n",
      "Epoch 484/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2276 - acc: 0.9248\n",
      "Epoch 485/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2474 - acc: 0.9124\n",
      "Epoch 486/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2573 - acc: 0.9171\n",
      "Epoch 487/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2558 - acc: 0.9124\n",
      "Epoch 488/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2699 - acc: 0.9129\n",
      "Epoch 489/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2347 - acc: 0.9233\n",
      "Epoch 490/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2588 - acc: 0.9200\n",
      "Epoch 491/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2539 - acc: 0.9148\n",
      "Epoch 492/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2685 - acc: 0.9076\n",
      "Epoch 493/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2753 - acc: 0.9043\n",
      "Epoch 494/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2449 - acc: 0.9190\n",
      "Epoch 495/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2483 - acc: 0.9138\n",
      "Epoch 496/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2516 - acc: 0.9200\n",
      "Epoch 497/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2564 - acc: 0.9105\n",
      "Epoch 498/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2596 - acc: 0.9114\n",
      "Epoch 499/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2449 - acc: 0.9200\n",
      "Epoch 500/500\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 0.2567 - acc: 0.9143\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(X_train, y_train, epochs = 500, batch_size = 50, \n",
    "          verbose = 1)## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82716QWAJrXG"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_8MFkQgYJm-D"
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('oscarwildemodel_500epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4AYVKydVJv5C"
   },
   "source": [
    "## If you have already trained the model and saved it, you can load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IqsQUz04J0GP"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('oscarwildemodel_500epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFe2Y0SJJ3Hb"
   },
   "source": [
    "### Note: After loading the model run  model.fit()  to continue training form there, if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "e9yLm_xnJ5JV"
   },
   "outputs": [],
   "source": [
    " model.fit(X_train, y_train, batch_size=50, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmkaxXdjHdyd"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "7RraFX9YHdye",
    "outputId": "87d6831d-075a-4995-dafa-6ed362301498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 4s 2ms/step\n",
      "[0.009652793156177116, 0.9957142846924918]\n",
      "\n",
      "Model Performance: Log Loss and Accuracy on validation data\n",
      "900/900 [==============================] - 1s 611us/step\n",
      "[10.47091334660848, 0.29555555747614964]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_train, y_train, batch_size = 20))\n",
    "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
    "print(model.evaluate(X_valid, y_valid, batch_size = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5CKxykLHdyj"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "4JSW5EwKHdyk",
    "outputId": "8b8f2e49-964e-4e54-ed4a-bac7739c3ed3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sequence: \n",
      "no the fact is i would like to be christened myself this afternoon if you have nothing better to do chasuble but surely mr worthing you have been christened already jack i don't remember anything about it chasuble but have you any grave doubts on the subject jack i certainly\n",
      "\n",
      "\n",
      "Generated Sequence: \n",
      "london i is in complete book form in 1891 by messrs so manager mr george alexander february 1907 sixth impression february 1907 sixth impression september 1905 fifth impression february 1907 sixth impression september 1905 fifth impression february 1907 sixth impression september 1905 fifth impression february 1907 sixth impression september 1905\n"
     ]
    }
   ],
   "source": [
    "seed_length=50\n",
    "new_words=50\n",
    "diversity=1\n",
    "n_gen=1\n",
    "\n",
    "import random\n",
    "\n",
    "# Choose a random sequence\n",
    "seq = random.choice(sequences)\n",
    "\n",
    "# print seq\n",
    "\n",
    "# Choose a random starting point\n",
    "seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
    "# Ending index for seed\n",
    "end_idx = seed_idx + seed_length\n",
    "\n",
    "gen_list = []\n",
    "\n",
    "for n in range(n_gen):\n",
    "    # Extract the seed sequence\n",
    "    seed = seq[seed_idx:end_idx]\n",
    "    original_sequence = [idx_word[i] for i in seed]\n",
    "    generated = seed[:] + ['#']\n",
    "\n",
    "    # Find the actual entire sequence\n",
    "    actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
    "        \n",
    "    # Keep adding new words\n",
    "    for i in range(new_words):\n",
    "\n",
    "        # Make a prediction from the seed\n",
    "        preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n",
    "\n",
    "        # Diversify\n",
    "        preds = np.log(preds) / diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "\n",
    "        # Softmax\n",
    "        preds = exp_preds / sum(exp_preds)\n",
    "\n",
    "        # Choose the next word\n",
    "        probas = np.random.multinomial(1, preds, 1)[0]\n",
    "\n",
    "        next_idx = np.argmax(probas)\n",
    "\n",
    "        # New seed adds on old word\n",
    "        #             seed = seed[1:] + [next_idx]\n",
    "        seed += [next_idx]\n",
    "        generated.append(next_idx)\n",
    "    # Showing generated and actual abstract\n",
    "    n = []\n",
    "\n",
    "    for i in generated:\n",
    "        n.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "    gen_list.append(n)\n",
    "\n",
    "a = []\n",
    "\n",
    "for i in actual:\n",
    "    a.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "a = a[seed_length:]\n",
    "\n",
    "gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
    "\n",
    "print('Original Sequence: \\n'+' '.join(original_sequence))\n",
    "print(\"\\n\")\n",
    "# print(gen_list)\n",
    "print('Generated Sequence: \\n'+' '.join(gen_list[0][1:]))\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9aXeiz7ZHdyp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "R9_External_Lab_Questions-2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
